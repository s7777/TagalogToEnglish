{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fA9Rv0qJh7g",
        "outputId": "1f743175-608d-40da-a648-1f1c7109a00b"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# load doc into memory\n",
        "\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = '/content/tgl.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-Tagalog.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-Tagalog.pkl\n",
            "[Run] => [Takbo]\n",
            "[Who] => [Sino]\n",
            "[Hello] => [Kamusta]\n",
            "[Hurry] => [Dali]\n",
            "[I try] => [Sinusubukan ko]\n",
            "[Smile] => [Ngiti]\n",
            "[Smile] => [Ngumiti ka]\n",
            "[Attack] => [Sugod]\n",
            "[He ran] => [Tumakbo siya]\n",
            "[Hug me] => [Yakapin mo ako]\n",
            "[I fell] => [Nahulog ako]\n",
            "[I know] => [Alam ko]\n",
            "[I work] => [Nagtatrabaho ako]\n",
            "[Really] => [Talaga]\n",
            "[Try it] => [Subukan ito]\n",
            "[We won] => [Nanalo kami]\n",
            "[Why me] => [Bakit ako]\n",
            "[Beat it] => [Alis]\n",
            "[Call me] => [Tawagan mo ako]\n",
            "[Call us] => [Tawagan mo kami]\n",
            "[Call us] => [Tawagan niyo kami]\n",
            "[Get out] => [Labas]\n",
            "[Get out] => [Lumabas ka]\n",
            "[Get out] => [Alis]\n",
            "[Go away] => [Umalis ka]\n",
            "[Go away] => [Umalis ka]\n",
            "[Go home] => [Umuwi ka]\n",
            "[He left] => [Umalis siya]\n",
            "[Help me] => [Tulungan mo ako]\n",
            "[Help us] => [Tulungan niyo kami]\n",
            "[Hug Tom] => [Yakapin mo si Tom]\n",
            "[I tried] => [Sinubukan ko]\n",
            "[Me too] => [Ako rin]\n",
            "[Show me] => [Ipakita sa akin]\n",
            "[Show me] => [Ipakita mo sa akin]\n",
            "[Show me] => [Ipakita niyo sa akin]\n",
            "[Stop it] => [Itigil iyan]\n",
            "[Take it] => [Kuhanin mo]\n",
            "[Take it] => [Kuhanin niyo]\n",
            "[Tom ran] => [Tumakbo si Tom]\n",
            "[Wait up] => [Intay]\n",
            "[Wake up] => [Gising]\n",
            "[Wake up] => [Gumising ka]\n",
            "[Wake up] => [Gumising kayo]\n",
            "[We lost] => [Natalo kami]\n",
            "[Who ate] => [Sinong kumain]\n",
            "[Who ran] => [Sinong tumakbo]\n",
            "[Why not] => [Bakit hindi]\n",
            "[You run] => [Tumatakbo ka]\n",
            "[You won] => [Nanalo ka]\n",
            "[Ask them] => [Tanungin mo sila]\n",
            "[Back off] => [Lumayo ka]\n",
            "[Be brave] => [Maging matapang ka]\n",
            "[Call Tom] => [Tawagin mo si Tom]\n",
            "[Call Tom] => [Tawagan mo si Tom]\n",
            "[Find Tom] => [Hanapin si Tom]\n",
            "[Fix this] => [Ayusin mo to]\n",
            "[Fix this] => [Ayusin mo ito]\n",
            "[Good job] => [Ang galing]\n",
            "[Grab him] => [Hablutin siya]\n",
            "[Have fun] => [Magpakasaya kayo]\n",
            "[Have fun] => [Magsaya ka]\n",
            "[Have fun] => [Magsaya kayo]\n",
            "[Help Tom] => [Tulungan mo si Tom]\n",
            "[How deep] => [Gaano kalalim]\n",
            "[I agreed] => [Sumangayon ako]\n",
            "[I got it] => [Nakuha ko na]\n",
            "[I stayed] => [Nanatili ako]\n",
            "[I waited] => [Nagintay ako]\n",
            "[Im back] => [Bumalik na ako]\n",
            "[Im done] => [Tapos na ako]\n",
            "[Im full] => [Busog na ako]\n",
            "[Im home] => [Nakauwi na ako]\n",
            "[It helps] => [Nakakatulong]\n",
            "[It hurts] => [Ang sakit]\n",
            "[Kiss Tom] => [Halikan mo si Tom]\n",
            "[Lets go] => [Tara]\n",
            "[Save Tom] => [Iligtas mo si Tom]\n",
            "[She left] => [Umalis siya]\n",
            "[Sit down] => [Upo]\n",
            "[Sit down] => [Umupo ka]\n",
            "[They won] => [Nanalo sila]\n",
            "[Tom came] => [Dumating si Tom]\n",
            "[Tom fell] => [Nahulog si Tom]\n",
            "[Tom left] => [Umalis si Tom]\n",
            "[Tom left] => [Wala na si Tom]\n",
            "[Tom lied] => [Nagsinungaling si Tom]\n",
            "[Tom lies] => [Nagsisinungaling si Tom]\n",
            "[Tom lost] => [Natalo si Tom]\n",
            "[Tom paid] => [Nagbayad si Tom]\n",
            "[Tom swam] => [Lumangoy si Tom]\n",
            "[Too late] => [Huli ka na]\n",
            "[Try this] => [Subukan ito]\n",
            "[Use this] => [Gamitin ito]\n",
            "[Use this] => [Gamitin mo ito]\n",
            "[Use this] => [Gamitin niyo ito]\n",
            "[Watch me] => [Panoorin ako]\n",
            "[Watch me] => [Panoorin mo ako]\n",
            "[Watch me] => [Panoorin niyo ako]\n",
            "[Were 1] => [Numero uno tayo]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAjeG_-GKTn_",
        "outputId": "603699a8-810c-4d66-8aaa-39b6570afb8c"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-Tagalog.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 3665\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:3000], dataset[665:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-Tagalog-both.pkl')\n",
        "save_clean_data(train, 'english-Tagalog-train.pkl')\n",
        "save_clean_data(test, 'english-Tagalog-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-Tagalog-both.pkl\n",
            "Saved: english-Tagalog-train.pkl\n",
            "Saved: english-Tagalog-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlakuOauLajs",
        "outputId": "d0723690-ae1e-42ab-fbeb-7e13fe50a0e3"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-Tagalog-both.pkl')\n",
        "train = load_clean_sentences('english-Tagalog-train.pkl')\n",
        "test = load_clean_sentences('english-Tagalog-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Tagalog Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Tagalog Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=400, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 2538\n",
            "English Max Length: 32\n",
            "Tagalog Vocabulary Size: 3229\n",
            "Tagalog Max Length: 33\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 33, 256)           826624    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 32, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 32, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 32, 2538)          652266    \n",
            "=================================================================\n",
            "Total params: 2,529,514\n",
            "Trainable params: 2,529,514\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/400\n",
            "47/47 - 51s - loss: 2.8490 - val_loss: 1.4233\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.42331, saving model to model.h5\n",
            "Epoch 2/400\n",
            "47/47 - 44s - loss: 1.2834 - val_loss: 1.2011\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.42331 to 1.20107, saving model to model.h5\n",
            "Epoch 3/400\n",
            "47/47 - 44s - loss: 1.2002 - val_loss: 1.1682\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.20107 to 1.16817, saving model to model.h5\n",
            "Epoch 4/400\n",
            "47/47 - 44s - loss: 1.1596 - val_loss: 1.1255\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.16817 to 1.12552, saving model to model.h5\n",
            "Epoch 5/400\n",
            "47/47 - 44s - loss: 1.1225 - val_loss: 1.1082\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.12552 to 1.10825, saving model to model.h5\n",
            "Epoch 6/400\n",
            "47/47 - 44s - loss: 1.1002 - val_loss: 1.0937\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.10825 to 1.09371, saving model to model.h5\n",
            "Epoch 7/400\n",
            "47/47 - 44s - loss: 1.0824 - val_loss: 1.0806\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.09371 to 1.08059, saving model to model.h5\n",
            "Epoch 8/400\n",
            "47/47 - 44s - loss: 1.0723 - val_loss: 1.0623\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.08059 to 1.06226, saving model to model.h5\n",
            "Epoch 9/400\n",
            "47/47 - 44s - loss: 1.0463 - val_loss: 1.0565\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.06226 to 1.05652, saving model to model.h5\n",
            "Epoch 10/400\n",
            "47/47 - 44s - loss: 1.0469 - val_loss: 1.0694\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.05652\n",
            "Epoch 11/400\n",
            "47/47 - 44s - loss: 1.0400 - val_loss: 1.0506\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.05652 to 1.05063, saving model to model.h5\n",
            "Epoch 12/400\n",
            "47/47 - 44s - loss: 1.0242 - val_loss: 1.0334\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.05063 to 1.03339, saving model to model.h5\n",
            "Epoch 13/400\n",
            "47/47 - 44s - loss: 1.0100 - val_loss: 1.0236\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.03339 to 1.02362, saving model to model.h5\n",
            "Epoch 14/400\n",
            "47/47 - 44s - loss: 1.0035 - val_loss: 1.0212\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.02362 to 1.02117, saving model to model.h5\n",
            "Epoch 15/400\n",
            "47/47 - 44s - loss: 0.9980 - val_loss: 1.0121\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.02117 to 1.01206, saving model to model.h5\n",
            "Epoch 16/400\n",
            "47/47 - 44s - loss: 0.9930 - val_loss: 1.0091\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.01206 to 1.00908, saving model to model.h5\n",
            "Epoch 17/400\n",
            "47/47 - 44s - loss: 0.9831 - val_loss: 1.0051\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.00908 to 1.00514, saving model to model.h5\n",
            "Epoch 18/400\n",
            "47/47 - 44s - loss: 0.9746 - val_loss: 1.0028\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.00514 to 1.00283, saving model to model.h5\n",
            "Epoch 19/400\n",
            "47/47 - 44s - loss: 0.9690 - val_loss: 0.9985\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.00283 to 0.99850, saving model to model.h5\n",
            "Epoch 20/400\n",
            "47/47 - 44s - loss: 0.9668 - val_loss: 0.9944\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.99850 to 0.99439, saving model to model.h5\n",
            "Epoch 21/400\n",
            "47/47 - 45s - loss: 0.9604 - val_loss: 0.9949\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.99439\n",
            "Epoch 22/400\n",
            "47/47 - 44s - loss: 0.9540 - val_loss: 0.9873\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.99439 to 0.98733, saving model to model.h5\n",
            "Epoch 23/400\n",
            "47/47 - 45s - loss: 0.9484 - val_loss: 0.9842\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.98733 to 0.98419, saving model to model.h5\n",
            "Epoch 24/400\n",
            "47/47 - 44s - loss: 0.9418 - val_loss: 0.9796\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.98419 to 0.97956, saving model to model.h5\n",
            "Epoch 25/400\n",
            "47/47 - 44s - loss: 0.9370 - val_loss: 0.9805\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.97956\n",
            "Epoch 26/400\n",
            "47/47 - 44s - loss: 0.9325 - val_loss: 0.9728\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.97956 to 0.97281, saving model to model.h5\n",
            "Epoch 27/400\n",
            "47/47 - 44s - loss: 0.9271 - val_loss: 0.9682\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.97281 to 0.96818, saving model to model.h5\n",
            "Epoch 28/400\n",
            "47/47 - 44s - loss: 0.9193 - val_loss: 0.9624\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.96818 to 0.96237, saving model to model.h5\n",
            "Epoch 29/400\n",
            "47/47 - 44s - loss: 0.9133 - val_loss: 0.9562\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.96237 to 0.95619, saving model to model.h5\n",
            "Epoch 30/400\n",
            "47/47 - 44s - loss: 0.9062 - val_loss: 0.9535\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.95619 to 0.95354, saving model to model.h5\n",
            "Epoch 31/400\n",
            "47/47 - 44s - loss: 0.8939 - val_loss: 0.9383\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.95354 to 0.93826, saving model to model.h5\n",
            "Epoch 32/400\n",
            "47/47 - 44s - loss: 0.8809 - val_loss: 0.9290\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.93826 to 0.92896, saving model to model.h5\n",
            "Epoch 33/400\n",
            "47/47 - 44s - loss: 0.8728 - val_loss: 0.9296\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.92896\n",
            "Epoch 34/400\n",
            "47/47 - 44s - loss: 0.8659 - val_loss: 0.9104\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.92896 to 0.91038, saving model to model.h5\n",
            "Epoch 35/400\n",
            "47/47 - 44s - loss: 0.8505 - val_loss: 0.9068\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.91038 to 0.90675, saving model to model.h5\n",
            "Epoch 36/400\n",
            "47/47 - 44s - loss: 0.8419 - val_loss: 0.9059\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.90675 to 0.90586, saving model to model.h5\n",
            "Epoch 37/400\n",
            "47/47 - 44s - loss: 0.8366 - val_loss: 0.8904\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.90586 to 0.89040, saving model to model.h5\n",
            "Epoch 38/400\n",
            "47/47 - 44s - loss: 0.8222 - val_loss: 0.8817\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.89040 to 0.88166, saving model to model.h5\n",
            "Epoch 39/400\n",
            "47/47 - 45s - loss: 0.8117 - val_loss: 0.8771\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.88166 to 0.87706, saving model to model.h5\n",
            "Epoch 40/400\n",
            "47/47 - 45s - loss: 0.8012 - val_loss: 0.8671\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.87706 to 0.86715, saving model to model.h5\n",
            "Epoch 41/400\n",
            "47/47 - 45s - loss: 0.7913 - val_loss: 0.8584\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.86715 to 0.85844, saving model to model.h5\n",
            "Epoch 42/400\n",
            "47/47 - 44s - loss: 0.7814 - val_loss: 0.8558\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.85844 to 0.85581, saving model to model.h5\n",
            "Epoch 43/400\n",
            "47/47 - 45s - loss: 0.7713 - val_loss: 0.8415\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.85581 to 0.84150, saving model to model.h5\n",
            "Epoch 44/400\n",
            "47/47 - 45s - loss: 0.7607 - val_loss: 0.8341\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.84150 to 0.83412, saving model to model.h5\n",
            "Epoch 45/400\n",
            "47/47 - 45s - loss: 0.7489 - val_loss: 0.8264\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.83412 to 0.82636, saving model to model.h5\n",
            "Epoch 46/400\n",
            "47/47 - 44s - loss: 0.7367 - val_loss: 0.8167\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.82636 to 0.81668, saving model to model.h5\n",
            "Epoch 47/400\n",
            "47/47 - 45s - loss: 0.7269 - val_loss: 0.8098\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.81668 to 0.80984, saving model to model.h5\n",
            "Epoch 48/400\n",
            "47/47 - 44s - loss: 0.7174 - val_loss: 0.8051\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.80984 to 0.80507, saving model to model.h5\n",
            "Epoch 49/400\n",
            "47/47 - 44s - loss: 0.7078 - val_loss: 0.7918\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.80507 to 0.79182, saving model to model.h5\n",
            "Epoch 50/400\n",
            "47/47 - 44s - loss: 0.6945 - val_loss: 0.7834\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.79182 to 0.78343, saving model to model.h5\n",
            "Epoch 51/400\n",
            "47/47 - 44s - loss: 0.6834 - val_loss: 0.7774\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.78343 to 0.77736, saving model to model.h5\n",
            "Epoch 52/400\n",
            "47/47 - 44s - loss: 0.6726 - val_loss: 0.7718\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.77736 to 0.77184, saving model to model.h5\n",
            "Epoch 53/400\n",
            "47/47 - 44s - loss: 0.6653 - val_loss: 0.7607\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.77184 to 0.76066, saving model to model.h5\n",
            "Epoch 54/400\n",
            "47/47 - 44s - loss: 0.6504 - val_loss: 0.7494\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.76066 to 0.74935, saving model to model.h5\n",
            "Epoch 55/400\n",
            "47/47 - 44s - loss: 0.6397 - val_loss: 0.7436\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.74935 to 0.74357, saving model to model.h5\n",
            "Epoch 56/400\n",
            "47/47 - 44s - loss: 0.6296 - val_loss: 0.7369\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.74357 to 0.73685, saving model to model.h5\n",
            "Epoch 57/400\n",
            "47/47 - 44s - loss: 0.6178 - val_loss: 0.7246\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.73685 to 0.72457, saving model to model.h5\n",
            "Epoch 58/400\n",
            "47/47 - 44s - loss: 0.6043 - val_loss: 0.7172\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.72457 to 0.71717, saving model to model.h5\n",
            "Epoch 59/400\n",
            "47/47 - 44s - loss: 0.5933 - val_loss: 0.7090\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.71717 to 0.70903, saving model to model.h5\n",
            "Epoch 60/400\n",
            "47/47 - 44s - loss: 0.5803 - val_loss: 0.6984\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.70903 to 0.69839, saving model to model.h5\n",
            "Epoch 61/400\n",
            "47/47 - 44s - loss: 0.5695 - val_loss: 0.6892\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.69839 to 0.68922, saving model to model.h5\n",
            "Epoch 62/400\n",
            "47/47 - 44s - loss: 0.5578 - val_loss: 0.6875\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.68922 to 0.68753, saving model to model.h5\n",
            "Epoch 63/400\n",
            "47/47 - 44s - loss: 0.5457 - val_loss: 0.6705\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.68753 to 0.67053, saving model to model.h5\n",
            "Epoch 64/400\n",
            "47/47 - 44s - loss: 0.5329 - val_loss: 0.6670\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.67053 to 0.66704, saving model to model.h5\n",
            "Epoch 65/400\n",
            "47/47 - 44s - loss: 0.5218 - val_loss: 0.6554\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.66704 to 0.65543, saving model to model.h5\n",
            "Epoch 66/400\n",
            "47/47 - 44s - loss: 0.5108 - val_loss: 0.6490\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.65543 to 0.64903, saving model to model.h5\n",
            "Epoch 67/400\n",
            "47/47 - 44s - loss: 0.5052 - val_loss: 0.6416\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.64903 to 0.64155, saving model to model.h5\n",
            "Epoch 68/400\n",
            "47/47 - 44s - loss: 0.4938 - val_loss: 0.6369\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.64155 to 0.63689, saving model to model.h5\n",
            "Epoch 69/400\n",
            "47/47 - 44s - loss: 0.4799 - val_loss: 0.6264\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.63689 to 0.62641, saving model to model.h5\n",
            "Epoch 70/400\n",
            "47/47 - 45s - loss: 0.4689 - val_loss: 0.6151\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.62641 to 0.61513, saving model to model.h5\n",
            "Epoch 71/400\n",
            "47/47 - 45s - loss: 0.4570 - val_loss: 0.6083\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.61513 to 0.60825, saving model to model.h5\n",
            "Epoch 72/400\n",
            "47/47 - 44s - loss: 0.4484 - val_loss: 0.6045\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.60825 to 0.60445, saving model to model.h5\n",
            "Epoch 73/400\n",
            "47/47 - 44s - loss: 0.4382 - val_loss: 0.5932\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.60445 to 0.59315, saving model to model.h5\n",
            "Epoch 74/400\n",
            "47/47 - 44s - loss: 0.4255 - val_loss: 0.5847\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.59315 to 0.58469, saving model to model.h5\n",
            "Epoch 75/400\n",
            "47/47 - 44s - loss: 0.4135 - val_loss: 0.5764\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.58469 to 0.57643, saving model to model.h5\n",
            "Epoch 76/400\n",
            "47/47 - 44s - loss: 0.4043 - val_loss: 0.5715\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.57643 to 0.57145, saving model to model.h5\n",
            "Epoch 77/400\n",
            "47/47 - 44s - loss: 0.3954 - val_loss: 0.5641\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.57145 to 0.56413, saving model to model.h5\n",
            "Epoch 78/400\n",
            "47/47 - 45s - loss: 0.3856 - val_loss: 0.5572\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.56413 to 0.55715, saving model to model.h5\n",
            "Epoch 79/400\n",
            "47/47 - 44s - loss: 0.3753 - val_loss: 0.5517\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.55715 to 0.55172, saving model to model.h5\n",
            "Epoch 80/400\n",
            "47/47 - 44s - loss: 0.3686 - val_loss: 0.5449\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.55172 to 0.54489, saving model to model.h5\n",
            "Epoch 81/400\n",
            "47/47 - 44s - loss: 0.3623 - val_loss: 0.5395\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.54489 to 0.53954, saving model to model.h5\n",
            "Epoch 82/400\n",
            "47/47 - 44s - loss: 0.3527 - val_loss: 0.5307\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.53954 to 0.53065, saving model to model.h5\n",
            "Epoch 83/400\n",
            "47/47 - 44s - loss: 0.3411 - val_loss: 0.5263\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.53065 to 0.52628, saving model to model.h5\n",
            "Epoch 84/400\n",
            "47/47 - 44s - loss: 0.3322 - val_loss: 0.5182\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.52628 to 0.51817, saving model to model.h5\n",
            "Epoch 85/400\n",
            "47/47 - 44s - loss: 0.3244 - val_loss: 0.5163\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.51817 to 0.51628, saving model to model.h5\n",
            "Epoch 86/400\n",
            "47/47 - 44s - loss: 0.3230 - val_loss: 0.5120\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.51628 to 0.51198, saving model to model.h5\n",
            "Epoch 87/400\n",
            "47/47 - 44s - loss: 0.3100 - val_loss: 0.5015\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.51198 to 0.50147, saving model to model.h5\n",
            "Epoch 88/400\n",
            "47/47 - 44s - loss: 0.3029 - val_loss: 0.5025\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.50147\n",
            "Epoch 89/400\n",
            "47/47 - 44s - loss: 0.2993 - val_loss: 0.4964\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.50147 to 0.49635, saving model to model.h5\n",
            "Epoch 90/400\n",
            "47/47 - 44s - loss: 0.2885 - val_loss: 0.4879\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.49635 to 0.48788, saving model to model.h5\n",
            "Epoch 91/400\n",
            "47/47 - 44s - loss: 0.2765 - val_loss: 0.4809\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.48788 to 0.48092, saving model to model.h5\n",
            "Epoch 92/400\n",
            "47/47 - 44s - loss: 0.2661 - val_loss: 0.4745\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.48092 to 0.47448, saving model to model.h5\n",
            "Epoch 93/400\n",
            "47/47 - 44s - loss: 0.2592 - val_loss: 0.4712\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.47448 to 0.47118, saving model to model.h5\n",
            "Epoch 94/400\n",
            "47/47 - 44s - loss: 0.2531 - val_loss: 0.4675\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.47118 to 0.46751, saving model to model.h5\n",
            "Epoch 95/400\n",
            "47/47 - 44s - loss: 0.2469 - val_loss: 0.4629\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.46751 to 0.46286, saving model to model.h5\n",
            "Epoch 96/400\n",
            "47/47 - 44s - loss: 0.2395 - val_loss: 0.4566\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.46286 to 0.45662, saving model to model.h5\n",
            "Epoch 97/400\n",
            "47/47 - 44s - loss: 0.2309 - val_loss: 0.4530\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.45662 to 0.45302, saving model to model.h5\n",
            "Epoch 98/400\n",
            "47/47 - 44s - loss: 0.2243 - val_loss: 0.4476\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.45302 to 0.44758, saving model to model.h5\n",
            "Epoch 99/400\n",
            "47/47 - 44s - loss: 0.2196 - val_loss: 0.4482\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.44758\n",
            "Epoch 100/400\n",
            "47/47 - 44s - loss: 0.2150 - val_loss: 0.4425\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.44758 to 0.44255, saving model to model.h5\n",
            "Epoch 101/400\n",
            "47/47 - 44s - loss: 0.2112 - val_loss: 0.4443\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.44255\n",
            "Epoch 102/400\n",
            "47/47 - 44s - loss: 0.2060 - val_loss: 0.4371\n",
            "\n",
            "Epoch 00102: val_loss improved from 0.44255 to 0.43709, saving model to model.h5\n",
            "Epoch 103/400\n",
            "47/47 - 44s - loss: 0.2028 - val_loss: 0.4343\n",
            "\n",
            "Epoch 00103: val_loss improved from 0.43709 to 0.43434, saving model to model.h5\n",
            "Epoch 104/400\n",
            "47/47 - 44s - loss: 0.1955 - val_loss: 0.4293\n",
            "\n",
            "Epoch 00104: val_loss improved from 0.43434 to 0.42927, saving model to model.h5\n",
            "Epoch 105/400\n",
            "47/47 - 44s - loss: 0.1878 - val_loss: 0.4231\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.42927 to 0.42313, saving model to model.h5\n",
            "Epoch 106/400\n",
            "47/47 - 44s - loss: 0.1815 - val_loss: 0.4205\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.42313 to 0.42046, saving model to model.h5\n",
            "Epoch 107/400\n",
            "47/47 - 45s - loss: 0.1767 - val_loss: 0.4194\n",
            "\n",
            "Epoch 00107: val_loss improved from 0.42046 to 0.41939, saving model to model.h5\n",
            "Epoch 108/400\n",
            "47/47 - 44s - loss: 0.1721 - val_loss: 0.4153\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.41939 to 0.41527, saving model to model.h5\n",
            "Epoch 109/400\n",
            "47/47 - 44s - loss: 0.1700 - val_loss: 0.4145\n",
            "\n",
            "Epoch 00109: val_loss improved from 0.41527 to 0.41451, saving model to model.h5\n",
            "Epoch 110/400\n",
            "47/47 - 44s - loss: 0.1709 - val_loss: 0.4160\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.41451\n",
            "Epoch 111/400\n",
            "47/47 - 44s - loss: 0.1662 - val_loss: 0.4170\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.41451\n",
            "Epoch 112/400\n",
            "47/47 - 44s - loss: 0.1572 - val_loss: 0.4053\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.41451 to 0.40530, saving model to model.h5\n",
            "Epoch 113/400\n",
            "47/47 - 44s - loss: 0.1502 - val_loss: 0.4025\n",
            "\n",
            "Epoch 00113: val_loss improved from 0.40530 to 0.40249, saving model to model.h5\n",
            "Epoch 114/400\n",
            "47/47 - 44s - loss: 0.1448 - val_loss: 0.3980\n",
            "\n",
            "Epoch 00114: val_loss improved from 0.40249 to 0.39799, saving model to model.h5\n",
            "Epoch 115/400\n",
            "47/47 - 44s - loss: 0.1400 - val_loss: 0.3946\n",
            "\n",
            "Epoch 00115: val_loss improved from 0.39799 to 0.39459, saving model to model.h5\n",
            "Epoch 116/400\n",
            "47/47 - 44s - loss: 0.1344 - val_loss: 0.3946\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.39459\n",
            "Epoch 117/400\n",
            "47/47 - 45s - loss: 0.1312 - val_loss: 0.3898\n",
            "\n",
            "Epoch 00117: val_loss improved from 0.39459 to 0.38982, saving model to model.h5\n",
            "Epoch 118/400\n",
            "47/47 - 44s - loss: 0.1280 - val_loss: 0.3887\n",
            "\n",
            "Epoch 00118: val_loss improved from 0.38982 to 0.38870, saving model to model.h5\n",
            "Epoch 119/400\n",
            "47/47 - 44s - loss: 0.1240 - val_loss: 0.3854\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.38870 to 0.38537, saving model to model.h5\n",
            "Epoch 120/400\n",
            "47/47 - 44s - loss: 0.1195 - val_loss: 0.3845\n",
            "\n",
            "Epoch 00120: val_loss improved from 0.38537 to 0.38450, saving model to model.h5\n",
            "Epoch 121/400\n",
            "47/47 - 44s - loss: 0.1180 - val_loss: 0.3868\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.38450\n",
            "Epoch 122/400\n",
            "47/47 - 44s - loss: 0.1196 - val_loss: 0.3877\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.38450\n",
            "Epoch 123/400\n",
            "47/47 - 44s - loss: 0.1188 - val_loss: 0.3913\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.38450\n",
            "Epoch 124/400\n",
            "47/47 - 45s - loss: 0.1148 - val_loss: 0.3855\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.38450\n",
            "Epoch 125/400\n",
            "47/47 - 45s - loss: 0.1125 - val_loss: 0.3843\n",
            "\n",
            "Epoch 00125: val_loss improved from 0.38450 to 0.38429, saving model to model.h5\n",
            "Epoch 126/400\n",
            "47/47 - 45s - loss: 0.1115 - val_loss: 0.3798\n",
            "\n",
            "Epoch 00126: val_loss improved from 0.38429 to 0.37983, saving model to model.h5\n",
            "Epoch 127/400\n",
            "47/47 - 45s - loss: 0.1079 - val_loss: 0.3787\n",
            "\n",
            "Epoch 00127: val_loss improved from 0.37983 to 0.37869, saving model to model.h5\n",
            "Epoch 128/400\n",
            "47/47 - 45s - loss: 0.1014 - val_loss: 0.3725\n",
            "\n",
            "Epoch 00128: val_loss improved from 0.37869 to 0.37251, saving model to model.h5\n",
            "Epoch 129/400\n",
            "47/47 - 45s - loss: 0.0953 - val_loss: 0.3698\n",
            "\n",
            "Epoch 00129: val_loss improved from 0.37251 to 0.36980, saving model to model.h5\n",
            "Epoch 130/400\n",
            "47/47 - 45s - loss: 0.0913 - val_loss: 0.3673\n",
            "\n",
            "Epoch 00130: val_loss improved from 0.36980 to 0.36731, saving model to model.h5\n",
            "Epoch 131/400\n",
            "47/47 - 45s - loss: 0.0867 - val_loss: 0.3657\n",
            "\n",
            "Epoch 00131: val_loss improved from 0.36731 to 0.36572, saving model to model.h5\n",
            "Epoch 132/400\n",
            "47/47 - 45s - loss: 0.0838 - val_loss: 0.3653\n",
            "\n",
            "Epoch 00132: val_loss improved from 0.36572 to 0.36527, saving model to model.h5\n",
            "Epoch 133/400\n",
            "47/47 - 44s - loss: 0.0820 - val_loss: 0.3649\n",
            "\n",
            "Epoch 00133: val_loss improved from 0.36527 to 0.36493, saving model to model.h5\n",
            "Epoch 134/400\n",
            "47/47 - 44s - loss: 0.0815 - val_loss: 0.3655\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.36493\n",
            "Epoch 135/400\n",
            "47/47 - 44s - loss: 0.0806 - val_loss: 0.3654\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.36493\n",
            "Epoch 136/400\n",
            "47/47 - 45s - loss: 0.0803 - val_loss: 0.3660\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.36493\n",
            "Epoch 137/400\n",
            "47/47 - 44s - loss: 0.0798 - val_loss: 0.3633\n",
            "\n",
            "Epoch 00137: val_loss improved from 0.36493 to 0.36330, saving model to model.h5\n",
            "Epoch 138/400\n",
            "47/47 - 44s - loss: 0.0779 - val_loss: 0.3652\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.36330\n",
            "Epoch 139/400\n",
            "47/47 - 45s - loss: 0.0778 - val_loss: 0.3637\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.36330\n",
            "Epoch 140/400\n",
            "47/47 - 45s - loss: 0.0739 - val_loss: 0.3589\n",
            "\n",
            "Epoch 00140: val_loss improved from 0.36330 to 0.35890, saving model to model.h5\n",
            "Epoch 141/400\n",
            "47/47 - 44s - loss: 0.0704 - val_loss: 0.3585\n",
            "\n",
            "Epoch 00141: val_loss improved from 0.35890 to 0.35847, saving model to model.h5\n",
            "Epoch 142/400\n",
            "47/47 - 44s - loss: 0.0670 - val_loss: 0.3603\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.35847\n",
            "Epoch 143/400\n",
            "47/47 - 45s - loss: 0.0649 - val_loss: 0.3582\n",
            "\n",
            "Epoch 00143: val_loss improved from 0.35847 to 0.35816, saving model to model.h5\n",
            "Epoch 144/400\n",
            "47/47 - 44s - loss: 0.0632 - val_loss: 0.3586\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.35816\n",
            "Epoch 145/400\n",
            "47/47 - 44s - loss: 0.0644 - val_loss: 0.3601\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.35816\n",
            "Epoch 146/400\n",
            "47/47 - 44s - loss: 0.0634 - val_loss: 0.3569\n",
            "\n",
            "Epoch 00146: val_loss improved from 0.35816 to 0.35693, saving model to model.h5\n",
            "Epoch 147/400\n",
            "47/47 - 44s - loss: 0.0601 - val_loss: 0.3550\n",
            "\n",
            "Epoch 00147: val_loss improved from 0.35693 to 0.35498, saving model to model.h5\n",
            "Epoch 148/400\n",
            "47/47 - 44s - loss: 0.0571 - val_loss: 0.3541\n",
            "\n",
            "Epoch 00148: val_loss improved from 0.35498 to 0.35413, saving model to model.h5\n",
            "Epoch 149/400\n",
            "47/47 - 44s - loss: 0.0548 - val_loss: 0.3523\n",
            "\n",
            "Epoch 00149: val_loss improved from 0.35413 to 0.35227, saving model to model.h5\n",
            "Epoch 150/400\n",
            "47/47 - 44s - loss: 0.0517 - val_loss: 0.3498\n",
            "\n",
            "Epoch 00150: val_loss improved from 0.35227 to 0.34985, saving model to model.h5\n",
            "Epoch 151/400\n",
            "47/47 - 44s - loss: 0.0495 - val_loss: 0.3508\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.34985\n",
            "Epoch 152/400\n",
            "47/47 - 44s - loss: 0.0496 - val_loss: 0.3515\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.34985\n",
            "Epoch 153/400\n",
            "47/47 - 44s - loss: 0.0499 - val_loss: 0.3514\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.34985\n",
            "Epoch 154/400\n",
            "47/47 - 44s - loss: 0.0500 - val_loss: 0.3541\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.34985\n",
            "Epoch 155/400\n",
            "47/47 - 44s - loss: 0.0487 - val_loss: 0.3514\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.34985\n",
            "Epoch 156/400\n",
            "47/47 - 44s - loss: 0.0482 - val_loss: 0.3511\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.34985\n",
            "Epoch 157/400\n",
            "47/47 - 44s - loss: 0.0477 - val_loss: 0.3551\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.34985\n",
            "Epoch 158/400\n",
            "47/47 - 43s - loss: 0.0559 - val_loss: 0.3610\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.34985\n",
            "Epoch 159/400\n",
            "47/47 - 44s - loss: 0.0593 - val_loss: 0.3611\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.34985\n",
            "Epoch 160/400\n",
            "47/47 - 44s - loss: 0.0584 - val_loss: 0.3582\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.34985\n",
            "Epoch 161/400\n",
            "47/47 - 44s - loss: 0.0540 - val_loss: 0.3569\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.34985\n",
            "Epoch 162/400\n",
            "47/47 - 44s - loss: 0.0496 - val_loss: 0.3520\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.34985\n",
            "Epoch 163/400\n",
            "47/47 - 44s - loss: 0.0449 - val_loss: 0.3518\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.34985\n",
            "Epoch 164/400\n",
            "47/47 - 44s - loss: 0.0409 - val_loss: 0.3488\n",
            "\n",
            "Epoch 00164: val_loss improved from 0.34985 to 0.34885, saving model to model.h5\n",
            "Epoch 165/400\n",
            "47/47 - 44s - loss: 0.0389 - val_loss: 0.3473\n",
            "\n",
            "Epoch 00165: val_loss improved from 0.34885 to 0.34728, saving model to model.h5\n",
            "Epoch 166/400\n",
            "47/47 - 44s - loss: 0.0372 - val_loss: 0.3490\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.34728\n",
            "Epoch 167/400\n",
            "47/47 - 44s - loss: 0.0351 - val_loss: 0.3462\n",
            "\n",
            "Epoch 00167: val_loss improved from 0.34728 to 0.34623, saving model to model.h5\n",
            "Epoch 168/400\n",
            "47/47 - 44s - loss: 0.0336 - val_loss: 0.3463\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.34623\n",
            "Epoch 169/400\n",
            "47/47 - 44s - loss: 0.0336 - val_loss: 0.3476\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.34623\n",
            "Epoch 170/400\n",
            "47/47 - 44s - loss: 0.0319 - val_loss: 0.3465\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.34623\n",
            "Epoch 171/400\n",
            "47/47 - 44s - loss: 0.0309 - val_loss: 0.3465\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.34623\n",
            "Epoch 172/400\n",
            "47/47 - 44s - loss: 0.0311 - val_loss: 0.3486\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.34623\n",
            "Epoch 173/400\n",
            "47/47 - 44s - loss: 0.0327 - val_loss: 0.3478\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.34623\n",
            "Epoch 174/400\n",
            "47/47 - 44s - loss: 0.0332 - val_loss: 0.3511\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.34623\n",
            "Epoch 175/400\n",
            "47/47 - 44s - loss: 0.0341 - val_loss: 0.3504\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.34623\n",
            "Epoch 176/400\n",
            "47/47 - 44s - loss: 0.0325 - val_loss: 0.3495\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.34623\n",
            "Epoch 177/400\n",
            "47/47 - 44s - loss: 0.0315 - val_loss: 0.3483\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.34623\n",
            "Epoch 178/400\n",
            "47/47 - 44s - loss: 0.0341 - val_loss: 0.3562\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.34623\n",
            "Epoch 179/400\n",
            "47/47 - 44s - loss: 0.0375 - val_loss: 0.3529\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.34623\n",
            "Epoch 180/400\n",
            "47/47 - 44s - loss: 0.0373 - val_loss: 0.3537\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.34623\n",
            "Epoch 181/400\n",
            "47/47 - 44s - loss: 0.0362 - val_loss: 0.3528\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.34623\n",
            "Epoch 182/400\n",
            "47/47 - 44s - loss: 0.0332 - val_loss: 0.3532\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.34623\n",
            "Epoch 183/400\n",
            "47/47 - 44s - loss: 0.0332 - val_loss: 0.3514\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.34623\n",
            "Epoch 184/400\n",
            "47/47 - 44s - loss: 0.0321 - val_loss: 0.3502\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.34623\n",
            "Epoch 185/400\n",
            "47/47 - 44s - loss: 0.0313 - val_loss: 0.3496\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.34623\n",
            "Epoch 186/400\n",
            "47/47 - 44s - loss: 0.0293 - val_loss: 0.3484\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.34623\n",
            "Epoch 187/400\n",
            "47/47 - 44s - loss: 0.0279 - val_loss: 0.3484\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.34623\n",
            "Epoch 188/400\n",
            "47/47 - 44s - loss: 0.0265 - val_loss: 0.3501\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.34623\n",
            "Epoch 189/400\n",
            "47/47 - 44s - loss: 0.0259 - val_loss: 0.3472\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.34623\n",
            "Epoch 190/400\n",
            "47/47 - 44s - loss: 0.0260 - val_loss: 0.3489\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.34623\n",
            "Epoch 191/400\n",
            "47/47 - 44s - loss: 0.0280 - val_loss: 0.3539\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.34623\n",
            "Epoch 192/400\n",
            "47/47 - 44s - loss: 0.0312 - val_loss: 0.3561\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.34623\n",
            "Epoch 193/400\n",
            "47/47 - 44s - loss: 0.0336 - val_loss: 0.3542\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.34623\n",
            "Epoch 194/400\n",
            "47/47 - 44s - loss: 0.0321 - val_loss: 0.3541\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.34623\n",
            "Epoch 195/400\n",
            "47/47 - 44s - loss: 0.0321 - val_loss: 0.3557\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.34623\n",
            "Epoch 196/400\n",
            "47/47 - 44s - loss: 0.0313 - val_loss: 0.3547\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.34623\n",
            "Epoch 197/400\n",
            "47/47 - 44s - loss: 0.0276 - val_loss: 0.3504\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.34623\n",
            "Epoch 198/400\n",
            "47/47 - 44s - loss: 0.0238 - val_loss: 0.3480\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.34623\n",
            "Epoch 199/400\n",
            "47/47 - 44s - loss: 0.0229 - val_loss: 0.3509\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.34623\n",
            "Epoch 200/400\n",
            "47/47 - 45s - loss: 0.0223 - val_loss: 0.3502\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.34623\n",
            "Epoch 201/400\n",
            "47/47 - 44s - loss: 0.0218 - val_loss: 0.3498\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.34623\n",
            "Epoch 202/400\n",
            "47/47 - 44s - loss: 0.0223 - val_loss: 0.3539\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.34623\n",
            "Epoch 203/400\n",
            "47/47 - 44s - loss: 0.0238 - val_loss: 0.3521\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.34623\n",
            "Epoch 204/400\n",
            "47/47 - 44s - loss: 0.0224 - val_loss: 0.3500\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.34623\n",
            "Epoch 205/400\n",
            "47/47 - 44s - loss: 0.0205 - val_loss: 0.3510\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.34623\n",
            "Epoch 206/400\n",
            "47/47 - 44s - loss: 0.0212 - val_loss: 0.3537\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.34623\n",
            "Epoch 207/400\n",
            "47/47 - 44s - loss: 0.0206 - val_loss: 0.3516\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.34623\n",
            "Epoch 208/400\n",
            "47/47 - 44s - loss: 0.0188 - val_loss: 0.3503\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.34623\n",
            "Epoch 209/400\n",
            "47/47 - 44s - loss: 0.0184 - val_loss: 0.3522\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.34623\n",
            "Epoch 210/400\n",
            "47/47 - 44s - loss: 0.0203 - val_loss: 0.3512\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.34623\n",
            "Epoch 211/400\n",
            "47/47 - 44s - loss: 0.0216 - val_loss: 0.3554\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.34623\n",
            "Epoch 212/400\n",
            "47/47 - 44s - loss: 0.0201 - val_loss: 0.3526\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.34623\n",
            "Epoch 213/400\n",
            "47/47 - 44s - loss: 0.0199 - val_loss: 0.3547\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.34623\n",
            "Epoch 214/400\n",
            "47/47 - 45s - loss: 0.0208 - val_loss: 0.3569\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.34623\n",
            "Epoch 215/400\n",
            "47/47 - 44s - loss: 0.0230 - val_loss: 0.3568\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.34623\n",
            "Epoch 216/400\n",
            "47/47 - 44s - loss: 0.0247 - val_loss: 0.3575\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.34623\n",
            "Epoch 217/400\n",
            "47/47 - 44s - loss: 0.0269 - val_loss: 0.3602\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.34623\n",
            "Epoch 218/400\n",
            "47/47 - 44s - loss: 0.0264 - val_loss: 0.3612\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.34623\n",
            "Epoch 219/400\n",
            "47/47 - 44s - loss: 0.0237 - val_loss: 0.3559\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.34623\n",
            "Epoch 220/400\n",
            "47/47 - 44s - loss: 0.0213 - val_loss: 0.3546\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.34623\n",
            "Epoch 221/400\n",
            "47/47 - 44s - loss: 0.0197 - val_loss: 0.3528\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.34623\n",
            "Epoch 222/400\n",
            "47/47 - 44s - loss: 0.0182 - val_loss: 0.3537\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.34623\n",
            "Epoch 223/400\n",
            "47/47 - 44s - loss: 0.0173 - val_loss: 0.3545\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.34623\n",
            "Epoch 224/400\n",
            "47/47 - 44s - loss: 0.0159 - val_loss: 0.3523\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.34623\n",
            "Epoch 225/400\n",
            "47/47 - 44s - loss: 0.0159 - val_loss: 0.3532\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.34623\n",
            "Epoch 226/400\n",
            "47/47 - 44s - loss: 0.0161 - val_loss: 0.3525\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.34623\n",
            "Epoch 227/400\n",
            "47/47 - 44s - loss: 0.0157 - val_loss: 0.3541\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.34623\n",
            "Epoch 228/400\n",
            "47/47 - 44s - loss: 0.0160 - val_loss: 0.3546\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.34623\n",
            "Epoch 229/400\n",
            "47/47 - 44s - loss: 0.0151 - val_loss: 0.3540\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.34623\n",
            "Epoch 230/400\n",
            "47/47 - 44s - loss: 0.0143 - val_loss: 0.3550\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.34623\n",
            "Epoch 231/400\n",
            "47/47 - 44s - loss: 0.0136 - val_loss: 0.3552\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.34623\n",
            "Epoch 232/400\n",
            "47/47 - 44s - loss: 0.0134 - val_loss: 0.3545\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.34623\n",
            "Epoch 233/400\n",
            "47/47 - 44s - loss: 0.0150 - val_loss: 0.3584\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.34623\n",
            "Epoch 234/400\n",
            "47/47 - 44s - loss: 0.0184 - val_loss: 0.3600\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.34623\n",
            "Epoch 235/400\n",
            "47/47 - 44s - loss: 0.0203 - val_loss: 0.3616\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.34623\n",
            "Epoch 236/400\n",
            "47/47 - 44s - loss: 0.0238 - val_loss: 0.3643\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.34623\n",
            "Epoch 237/400\n",
            "47/47 - 44s - loss: 0.0259 - val_loss: 0.3680\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.34623\n",
            "Epoch 238/400\n",
            "47/47 - 44s - loss: 0.0313 - val_loss: 0.3720\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.34623\n",
            "Epoch 239/400\n",
            "47/47 - 44s - loss: 0.0433 - val_loss: 0.3863\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.34623\n",
            "Epoch 240/400\n",
            "47/47 - 44s - loss: 0.0466 - val_loss: 0.3733\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.34623\n",
            "Epoch 241/400\n",
            "47/47 - 44s - loss: 0.0409 - val_loss: 0.3675\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.34623\n",
            "Epoch 242/400\n",
            "47/47 - 44s - loss: 0.0306 - val_loss: 0.3621\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.34623\n",
            "Epoch 243/400\n",
            "47/47 - 44s - loss: 0.0249 - val_loss: 0.3608\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.34623\n",
            "Epoch 244/400\n",
            "47/47 - 44s - loss: 0.0224 - val_loss: 0.3573\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.34623\n",
            "Epoch 245/400\n",
            "47/47 - 44s - loss: 0.0191 - val_loss: 0.3556\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.34623\n",
            "Epoch 246/400\n",
            "47/47 - 44s - loss: 0.0150 - val_loss: 0.3544\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.34623\n",
            "Epoch 247/400\n",
            "47/47 - 44s - loss: 0.0130 - val_loss: 0.3538\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.34623\n",
            "Epoch 248/400\n",
            "47/47 - 44s - loss: 0.0118 - val_loss: 0.3529\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.34623\n",
            "Epoch 249/400\n",
            "47/47 - 44s - loss: 0.0113 - val_loss: 0.3536\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.34623\n",
            "Epoch 250/400\n",
            "47/47 - 44s - loss: 0.0114 - val_loss: 0.3542\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.34623\n",
            "Epoch 251/400\n",
            "47/47 - 45s - loss: 0.0114 - val_loss: 0.3546\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.34623\n",
            "Epoch 252/400\n",
            "47/47 - 45s - loss: 0.0117 - val_loss: 0.3531\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.34623\n",
            "Epoch 253/400\n",
            "47/47 - 44s - loss: 0.0124 - val_loss: 0.3559\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.34623\n",
            "Epoch 254/400\n",
            "47/47 - 44s - loss: 0.0139 - val_loss: 0.3582\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.34623\n",
            "Epoch 255/400\n",
            "47/47 - 44s - loss: 0.0176 - val_loss: 0.3698\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.34623\n",
            "Epoch 256/400\n",
            "47/47 - 44s - loss: 0.0197 - val_loss: 0.3609\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.34623\n",
            "Epoch 257/400\n",
            "47/47 - 44s - loss: 0.0163 - val_loss: 0.3581\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.34623\n",
            "Epoch 258/400\n",
            "47/47 - 44s - loss: 0.0130 - val_loss: 0.3557\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.34623\n",
            "Epoch 259/400\n",
            "47/47 - 44s - loss: 0.0111 - val_loss: 0.3569\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.34623\n",
            "Epoch 260/400\n",
            "47/47 - 44s - loss: 0.0106 - val_loss: 0.3563\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.34623\n",
            "Epoch 261/400\n",
            "47/47 - 45s - loss: 0.0109 - val_loss: 0.3553\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.34623\n",
            "Epoch 262/400\n",
            "47/47 - 45s - loss: 0.0108 - val_loss: 0.3553\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.34623\n",
            "Epoch 263/400\n",
            "47/47 - 44s - loss: 0.0102 - val_loss: 0.3558\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.34623\n",
            "Epoch 264/400\n",
            "47/47 - 44s - loss: 0.0105 - val_loss: 0.3576\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.34623\n",
            "Epoch 265/400\n",
            "47/47 - 44s - loss: 0.0112 - val_loss: 0.3580\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.34623\n",
            "Epoch 266/400\n",
            "47/47 - 44s - loss: 0.0116 - val_loss: 0.3567\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.34623\n",
            "Epoch 267/400\n",
            "47/47 - 45s - loss: 0.0121 - val_loss: 0.3579\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.34623\n",
            "Epoch 268/400\n",
            "47/47 - 45s - loss: 0.0116 - val_loss: 0.3590\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.34623\n",
            "Epoch 269/400\n",
            "47/47 - 45s - loss: 0.0113 - val_loss: 0.3585\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.34623\n",
            "Epoch 270/400\n",
            "47/47 - 44s - loss: 0.0115 - val_loss: 0.3592\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.34623\n",
            "Epoch 271/400\n",
            "47/47 - 44s - loss: 0.0116 - val_loss: 0.3589\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.34623\n",
            "Epoch 272/400\n",
            "47/47 - 45s - loss: 0.0146 - val_loss: 0.3688\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.34623\n",
            "Epoch 273/400\n",
            "47/47 - 45s - loss: 0.0207 - val_loss: 0.3674\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.34623\n",
            "Epoch 274/400\n",
            "47/47 - 45s - loss: 0.0209 - val_loss: 0.3669\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.34623\n",
            "Epoch 275/400\n",
            "47/47 - 44s - loss: 0.0201 - val_loss: 0.3670\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.34623\n",
            "Epoch 276/400\n",
            "47/47 - 44s - loss: 0.0198 - val_loss: 0.3665\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.34623\n",
            "Epoch 277/400\n",
            "47/47 - 44s - loss: 0.0246 - val_loss: 0.3741\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.34623\n",
            "Epoch 278/400\n",
            "47/47 - 44s - loss: 0.0285 - val_loss: 0.3719\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.34623\n",
            "Epoch 279/400\n",
            "47/47 - 44s - loss: 0.0237 - val_loss: 0.3685\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.34623\n",
            "Epoch 280/400\n",
            "47/47 - 44s - loss: 0.0187 - val_loss: 0.3654\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.34623\n",
            "Epoch 281/400\n",
            "47/47 - 44s - loss: 0.0155 - val_loss: 0.3641\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.34623\n",
            "Epoch 282/400\n",
            "47/47 - 44s - loss: 0.0165 - val_loss: 0.3652\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.34623\n",
            "Epoch 283/400\n",
            "47/47 - 44s - loss: 0.0154 - val_loss: 0.3626\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.34623\n",
            "Epoch 284/400\n",
            "47/47 - 44s - loss: 0.0127 - val_loss: 0.3597\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.34623\n",
            "Epoch 285/400\n",
            "47/47 - 44s - loss: 0.0104 - val_loss: 0.3614\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.34623\n",
            "Epoch 286/400\n",
            "47/47 - 44s - loss: 0.0099 - val_loss: 0.3599\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.34623\n",
            "Epoch 287/400\n",
            "47/47 - 44s - loss: 0.0097 - val_loss: 0.3601\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.34623\n",
            "Epoch 288/400\n",
            "47/47 - 44s - loss: 0.0112 - val_loss: 0.3617\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.34623\n",
            "Epoch 289/400\n",
            "47/47 - 44s - loss: 0.0117 - val_loss: 0.3643\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.34623\n",
            "Epoch 290/400\n",
            "47/47 - 44s - loss: 0.0106 - val_loss: 0.3630\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.34623\n",
            "Epoch 291/400\n",
            "47/47 - 44s - loss: 0.0101 - val_loss: 0.3611\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.34623\n",
            "Epoch 292/400\n",
            "47/47 - 44s - loss: 0.0105 - val_loss: 0.3618\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.34623\n",
            "Epoch 293/400\n",
            "47/47 - 44s - loss: 0.0101 - val_loss: 0.3620\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.34623\n",
            "Epoch 294/400\n",
            "47/47 - 44s - loss: 0.0096 - val_loss: 0.3632\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.34623\n",
            "Epoch 295/400\n",
            "47/47 - 44s - loss: 0.0100 - val_loss: 0.3615\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.34623\n",
            "Epoch 296/400\n",
            "47/47 - 44s - loss: 0.0092 - val_loss: 0.3625\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.34623\n",
            "Epoch 297/400\n",
            "47/47 - 44s - loss: 0.0093 - val_loss: 0.3634\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.34623\n",
            "Epoch 298/400\n",
            "47/47 - 44s - loss: 0.0106 - val_loss: 0.3641\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.34623\n",
            "Epoch 299/400\n",
            "47/47 - 44s - loss: 0.0108 - val_loss: 0.3628\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.34623\n",
            "Epoch 300/400\n",
            "47/47 - 44s - loss: 0.0103 - val_loss: 0.3669\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.34623\n",
            "Epoch 301/400\n",
            "47/47 - 45s - loss: 0.0126 - val_loss: 0.3681\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.34623\n",
            "Epoch 302/400\n",
            "47/47 - 44s - loss: 0.0150 - val_loss: 0.3678\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.34623\n",
            "Epoch 303/400\n",
            "47/47 - 44s - loss: 0.0151 - val_loss: 0.3720\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.34623\n",
            "Epoch 304/400\n",
            "47/47 - 44s - loss: 0.0162 - val_loss: 0.3683\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.34623\n",
            "Epoch 305/400\n",
            "47/47 - 44s - loss: 0.0184 - val_loss: 0.3709\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.34623\n",
            "Epoch 306/400\n",
            "47/47 - 44s - loss: 0.0196 - val_loss: 0.3722\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.34623\n",
            "Epoch 307/400\n",
            "47/47 - 44s - loss: 0.0217 - val_loss: 0.3731\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.34623\n",
            "Epoch 308/400\n",
            "47/47 - 45s - loss: 0.0230 - val_loss: 0.3715\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.34623\n",
            "Epoch 309/400\n",
            "47/47 - 44s - loss: 0.0209 - val_loss: 0.3724\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.34623\n",
            "Epoch 310/400\n",
            "47/47 - 44s - loss: 0.0180 - val_loss: 0.3683\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.34623\n",
            "Epoch 311/400\n",
            "47/47 - 44s - loss: 0.0140 - val_loss: 0.3657\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.34623\n",
            "Epoch 312/400\n",
            "47/47 - 44s - loss: 0.0113 - val_loss: 0.3629\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.34623\n",
            "Epoch 313/400\n",
            "47/47 - 44s - loss: 0.0096 - val_loss: 0.3645\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.34623\n",
            "Epoch 314/400\n",
            "47/47 - 44s - loss: 0.0090 - val_loss: 0.3649\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.34623\n",
            "Epoch 315/400\n",
            "47/47 - 44s - loss: 0.0085 - val_loss: 0.3643\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.34623\n",
            "Epoch 316/400\n",
            "47/47 - 44s - loss: 0.0081 - val_loss: 0.3637\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.34623\n",
            "Epoch 317/400\n",
            "47/47 - 45s - loss: 0.0083 - val_loss: 0.3635\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.34623\n",
            "Epoch 318/400\n",
            "47/47 - 44s - loss: 0.0085 - val_loss: 0.3662\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.34623\n",
            "Epoch 319/400\n",
            "47/47 - 44s - loss: 0.0093 - val_loss: 0.3663\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.34623\n",
            "Epoch 320/400\n",
            "47/47 - 44s - loss: 0.0106 - val_loss: 0.3681\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.34623\n",
            "Epoch 321/400\n",
            "47/47 - 44s - loss: 0.0115 - val_loss: 0.3680\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.34623\n",
            "Epoch 322/400\n",
            "47/47 - 45s - loss: 0.0126 - val_loss: 0.3693\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.34623\n",
            "Epoch 323/400\n",
            "47/47 - 44s - loss: 0.0119 - val_loss: 0.3672\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.34623\n",
            "Epoch 324/400\n",
            "47/47 - 44s - loss: 0.0114 - val_loss: 0.3699\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.34623\n",
            "Epoch 325/400\n",
            "47/47 - 44s - loss: 0.0108 - val_loss: 0.3663\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.34623\n",
            "Epoch 326/400\n",
            "47/47 - 44s - loss: 0.0098 - val_loss: 0.3668\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.34623\n",
            "Epoch 327/400\n",
            "47/47 - 44s - loss: 0.0087 - val_loss: 0.3665\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.34623\n",
            "Epoch 328/400\n",
            "47/47 - 45s - loss: 0.0086 - val_loss: 0.3663\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.34623\n",
            "Epoch 329/400\n",
            "47/47 - 45s - loss: 0.0080 - val_loss: 0.3666\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.34623\n",
            "Epoch 330/400\n",
            "47/47 - 44s - loss: 0.0079 - val_loss: 0.3667\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.34623\n",
            "Epoch 331/400\n",
            "47/47 - 44s - loss: 0.0078 - val_loss: 0.3666\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.34623\n",
            "Epoch 332/400\n",
            "47/47 - 44s - loss: 0.0079 - val_loss: 0.3662\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.34623\n",
            "Epoch 333/400\n",
            "47/47 - 44s - loss: 0.0090 - val_loss: 0.3681\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.34623\n",
            "Epoch 334/400\n",
            "47/47 - 44s - loss: 0.0100 - val_loss: 0.3735\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.34623\n",
            "Epoch 335/400\n",
            "47/47 - 45s - loss: 0.0122 - val_loss: 0.3704\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.34623\n",
            "Epoch 336/400\n",
            "47/47 - 45s - loss: 0.0111 - val_loss: 0.3716\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.34623\n",
            "Epoch 337/400\n",
            "47/47 - 44s - loss: 0.0158 - val_loss: 0.3779\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.34623\n",
            "Epoch 338/400\n",
            "47/47 - 45s - loss: 0.0226 - val_loss: 0.3787\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.34623\n",
            "Epoch 339/400\n",
            "47/47 - 45s - loss: 0.0241 - val_loss: 0.3870\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.34623\n",
            "Epoch 340/400\n",
            "47/47 - 45s - loss: 0.0290 - val_loss: 0.3840\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.34623\n",
            "Epoch 341/400\n",
            "47/47 - 45s - loss: 0.0272 - val_loss: 0.3803\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.34623\n",
            "Epoch 342/400\n",
            "47/47 - 45s - loss: 0.0214 - val_loss: 0.3732\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.34623\n",
            "Epoch 343/400\n",
            "47/47 - 45s - loss: 0.0186 - val_loss: 0.3758\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.34623\n",
            "Epoch 344/400\n",
            "47/47 - 45s - loss: 0.0138 - val_loss: 0.3680\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.34623\n",
            "Epoch 345/400\n",
            "47/47 - 45s - loss: 0.0121 - val_loss: 0.3690\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.34623\n",
            "Epoch 346/400\n",
            "47/47 - 45s - loss: 0.0101 - val_loss: 0.3681\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.34623\n",
            "Epoch 347/400\n",
            "47/47 - 45s - loss: 0.0083 - val_loss: 0.3660\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.34623\n",
            "Epoch 348/400\n",
            "47/47 - 45s - loss: 0.0077 - val_loss: 0.3662\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.34623\n",
            "Epoch 349/400\n",
            "47/47 - 45s - loss: 0.0077 - val_loss: 0.3659\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.34623\n",
            "Epoch 350/400\n",
            "47/47 - 45s - loss: 0.0073 - val_loss: 0.3671\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.34623\n",
            "Epoch 351/400\n",
            "47/47 - 45s - loss: 0.0073 - val_loss: 0.3675\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.34623\n",
            "Epoch 352/400\n",
            "47/47 - 45s - loss: 0.0073 - val_loss: 0.3670\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.34623\n",
            "Epoch 353/400\n",
            "47/47 - 45s - loss: 0.0075 - val_loss: 0.3670\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.34623\n",
            "Epoch 354/400\n",
            "47/47 - 44s - loss: 0.0078 - val_loss: 0.3681\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.34623\n",
            "Epoch 355/400\n",
            "47/47 - 45s - loss: 0.0085 - val_loss: 0.3694\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.34623\n",
            "Epoch 356/400\n",
            "47/47 - 45s - loss: 0.0088 - val_loss: 0.3691\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.34623\n",
            "Epoch 357/400\n",
            "47/47 - 44s - loss: 0.0089 - val_loss: 0.3686\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.34623\n",
            "Epoch 358/400\n",
            "47/47 - 45s - loss: 0.0085 - val_loss: 0.3677\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.34623\n",
            "Epoch 359/400\n",
            "47/47 - 44s - loss: 0.0084 - val_loss: 0.3703\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.34623\n",
            "Epoch 360/400\n",
            "47/47 - 45s - loss: 0.0086 - val_loss: 0.3688\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.34623\n",
            "Epoch 361/400\n",
            "47/47 - 45s - loss: 0.0084 - val_loss: 0.3722\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.34623\n",
            "Epoch 362/400\n",
            "47/47 - 45s - loss: 0.0091 - val_loss: 0.3729\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.34623\n",
            "Epoch 363/400\n",
            "47/47 - 45s - loss: 0.0094 - val_loss: 0.3700\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.34623\n",
            "Epoch 364/400\n",
            "47/47 - 45s - loss: 0.0090 - val_loss: 0.3710\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.34623\n",
            "Epoch 365/400\n",
            "47/47 - 45s - loss: 0.0104 - val_loss: 0.3749\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.34623\n",
            "Epoch 366/400\n",
            "47/47 - 44s - loss: 0.0137 - val_loss: 0.3791\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.34623\n",
            "Epoch 367/400\n",
            "47/47 - 45s - loss: 0.0159 - val_loss: 0.3746\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.34623\n",
            "Epoch 368/400\n",
            "47/47 - 45s - loss: 0.0169 - val_loss: 0.3774\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.34623\n",
            "Epoch 369/400\n",
            "47/47 - 45s - loss: 0.0148 - val_loss: 0.3746\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.34623\n",
            "Epoch 370/400\n",
            "47/47 - 45s - loss: 0.0130 - val_loss: 0.3727\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.34623\n",
            "Epoch 371/400\n",
            "47/47 - 45s - loss: 0.0129 - val_loss: 0.3777\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.34623\n",
            "Epoch 372/400\n",
            "47/47 - 45s - loss: 0.0131 - val_loss: 0.3755\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.34623\n",
            "Epoch 373/400\n",
            "47/47 - 45s - loss: 0.0135 - val_loss: 0.3775\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.34623\n",
            "Epoch 374/400\n",
            "47/47 - 45s - loss: 0.0145 - val_loss: 0.3748\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.34623\n",
            "Epoch 375/400\n",
            "47/47 - 45s - loss: 0.0145 - val_loss: 0.3747\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.34623\n",
            "Epoch 376/400\n",
            "47/47 - 45s - loss: 0.0130 - val_loss: 0.3757\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.34623\n",
            "Epoch 377/400\n",
            "47/47 - 45s - loss: 0.0105 - val_loss: 0.3709\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.34623\n",
            "Epoch 378/400\n",
            "47/47 - 45s - loss: 0.0092 - val_loss: 0.3719\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.34623\n",
            "Epoch 379/400\n",
            "47/47 - 45s - loss: 0.0083 - val_loss: 0.3718\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.34623\n",
            "Epoch 380/400\n",
            "47/47 - 45s - loss: 0.0079 - val_loss: 0.3711\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.34623\n",
            "Epoch 381/400\n",
            "47/47 - 45s - loss: 0.0076 - val_loss: 0.3729\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.34623\n",
            "Epoch 382/400\n",
            "47/47 - 45s - loss: 0.0077 - val_loss: 0.3711\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.34623\n",
            "Epoch 383/400\n",
            "47/47 - 45s - loss: 0.0073 - val_loss: 0.3716\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.34623\n",
            "Epoch 384/400\n",
            "47/47 - 45s - loss: 0.0074 - val_loss: 0.3719\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.34623\n",
            "Epoch 385/400\n",
            "47/47 - 44s - loss: 0.0070 - val_loss: 0.3729\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.34623\n",
            "Epoch 386/400\n",
            "47/47 - 45s - loss: 0.0075 - val_loss: 0.3725\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.34623\n",
            "Epoch 387/400\n",
            "47/47 - 45s - loss: 0.0078 - val_loss: 0.3719\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.34623\n",
            "Epoch 388/400\n",
            "47/47 - 45s - loss: 0.0096 - val_loss: 0.3741\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.34623\n",
            "Epoch 389/400\n",
            "47/47 - 45s - loss: 0.0130 - val_loss: 0.3799\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.34623\n",
            "Epoch 390/400\n",
            "47/47 - 45s - loss: 0.0132 - val_loss: 0.3751\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.34623\n",
            "Epoch 391/400\n",
            "47/47 - 44s - loss: 0.0130 - val_loss: 0.3791\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.34623\n",
            "Epoch 392/400\n",
            "47/47 - 45s - loss: 0.0148 - val_loss: 0.3760\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.34623\n",
            "Epoch 393/400\n",
            "47/47 - 45s - loss: 0.0151 - val_loss: 0.3778\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.34623\n",
            "Epoch 394/400\n",
            "47/47 - 45s - loss: 0.0127 - val_loss: 0.3755\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.34623\n",
            "Epoch 395/400\n",
            "47/47 - 45s - loss: 0.0101 - val_loss: 0.3746\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.34623\n",
            "Epoch 396/400\n",
            "47/47 - 45s - loss: 0.0083 - val_loss: 0.3768\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.34623\n",
            "Epoch 397/400\n",
            "47/47 - 44s - loss: 0.0079 - val_loss: 0.3725\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.34623\n",
            "Epoch 398/400\n",
            "47/47 - 44s - loss: 0.0070 - val_loss: 0.3751\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.34623\n",
            "Epoch 399/400\n",
            "47/47 - 44s - loss: 0.0065 - val_loss: 0.3741\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.34623\n",
            "Epoch 400/400\n",
            "47/47 - 45s - loss: 0.0068 - val_loss: 0.3737\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.34623\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ffb397be8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG_3P_Bfz-DU",
        "outputId": "c64e7bfb-55b9-4eff-c6f7-e142a599fc93"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-Tagalog-both.pkl')\n",
        "train = load_clean_sentences('english-Tagalog-train.pkl')\n",
        "test = load_clean_sentences('english-Tagalog-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Tagalog Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Tagalog Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=200, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 2538\n",
            "English Max Length: 32\n",
            "Tagalog Vocabulary Size: 3229\n",
            "Tagalog Max Length: 33\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 33, 256)           826624    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 32, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 32, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 32, 2538)          652266    \n",
            "=================================================================\n",
            "Total params: 2,529,514\n",
            "Trainable params: 2,529,514\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "47/47 - 47s - loss: 2.8207 - val_loss: 1.4181\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.41812, saving model to model.h5\n",
            "Epoch 2/200\n",
            "47/47 - 40s - loss: 1.2835 - val_loss: 1.2216\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.41812 to 1.22158, saving model to model.h5\n",
            "Epoch 3/200\n",
            "47/47 - 40s - loss: 1.2197 - val_loss: 1.1732\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.22158 to 1.17316, saving model to model.h5\n",
            "Epoch 4/200\n",
            "47/47 - 40s - loss: 1.1593 - val_loss: 1.1339\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.17316 to 1.13391, saving model to model.h5\n",
            "Epoch 5/200\n",
            "47/47 - 40s - loss: 1.1317 - val_loss: 1.1199\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.13391 to 1.11989, saving model to model.h5\n",
            "Epoch 6/200\n",
            "47/47 - 40s - loss: 1.1301 - val_loss: 1.1024\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.11989 to 1.10236, saving model to model.h5\n",
            "Epoch 7/200\n",
            "47/47 - 40s - loss: 1.0920 - val_loss: 1.0818\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.10236 to 1.08181, saving model to model.h5\n",
            "Epoch 8/200\n",
            "47/47 - 40s - loss: 1.0671 - val_loss: 1.0667\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.08181 to 1.06669, saving model to model.h5\n",
            "Epoch 9/200\n",
            "47/47 - 40s - loss: 1.0530 - val_loss: 1.0602\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.06669 to 1.06020, saving model to model.h5\n",
            "Epoch 10/200\n",
            "47/47 - 40s - loss: 1.0432 - val_loss: 1.0596\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.06020 to 1.05958, saving model to model.h5\n",
            "Epoch 11/200\n",
            "47/47 - 43s - loss: 1.0338 - val_loss: 1.0390\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.05958 to 1.03902, saving model to model.h5\n",
            "Epoch 12/200\n",
            "47/47 - 40s - loss: 1.0202 - val_loss: 1.0335\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.03902 to 1.03353, saving model to model.h5\n",
            "Epoch 13/200\n",
            "47/47 - 40s - loss: 1.0134 - val_loss: 1.0227\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.03353 to 1.02266, saving model to model.h5\n",
            "Epoch 14/200\n",
            "47/47 - 40s - loss: 1.0046 - val_loss: 1.0188\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.02266 to 1.01882, saving model to model.h5\n",
            "Epoch 15/200\n",
            "47/47 - 40s - loss: 0.9978 - val_loss: 1.0192\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.01882\n",
            "Epoch 16/200\n",
            "47/47 - 40s - loss: 0.9904 - val_loss: 1.0121\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.01882 to 1.01209, saving model to model.h5\n",
            "Epoch 17/200\n",
            "47/47 - 40s - loss: 0.9821 - val_loss: 1.0118\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.01209 to 1.01183, saving model to model.h5\n",
            "Epoch 18/200\n",
            "47/47 - 40s - loss: 0.9774 - val_loss: 1.0089\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.01183 to 1.00888, saving model to model.h5\n",
            "Epoch 19/200\n",
            "47/47 - 40s - loss: 0.9731 - val_loss: 1.0043\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.00888 to 1.00432, saving model to model.h5\n",
            "Epoch 20/200\n",
            "47/47 - 40s - loss: 0.9666 - val_loss: 1.0017\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.00432 to 1.00170, saving model to model.h5\n",
            "Epoch 21/200\n",
            "47/47 - 40s - loss: 0.9600 - val_loss: 0.9989\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.00170 to 0.99889, saving model to model.h5\n",
            "Epoch 22/200\n",
            "47/47 - 40s - loss: 0.9596 - val_loss: 0.9965\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.99889 to 0.99647, saving model to model.h5\n",
            "Epoch 23/200\n",
            "47/47 - 40s - loss: 0.9548 - val_loss: 0.9943\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.99647 to 0.99431, saving model to model.h5\n",
            "Epoch 24/200\n",
            "47/47 - 40s - loss: 0.9503 - val_loss: 0.9883\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.99431 to 0.98826, saving model to model.h5\n",
            "Epoch 25/200\n",
            "47/47 - 40s - loss: 0.9428 - val_loss: 0.9832\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.98826 to 0.98324, saving model to model.h5\n",
            "Epoch 26/200\n",
            "47/47 - 40s - loss: 0.9373 - val_loss: 0.9834\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.98324\n",
            "Epoch 27/200\n",
            "47/47 - 40s - loss: 0.9322 - val_loss: 0.9731\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.98324 to 0.97310, saving model to model.h5\n",
            "Epoch 28/200\n",
            "47/47 - 40s - loss: 0.9238 - val_loss: 0.9655\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.97310 to 0.96549, saving model to model.h5\n",
            "Epoch 29/200\n",
            "47/47 - 41s - loss: 0.9146 - val_loss: 0.9591\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.96549 to 0.95913, saving model to model.h5\n",
            "Epoch 30/200\n",
            "47/47 - 41s - loss: 0.9045 - val_loss: 0.9544\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.95913 to 0.95439, saving model to model.h5\n",
            "Epoch 31/200\n",
            "47/47 - 40s - loss: 0.8953 - val_loss: 0.9472\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.95439 to 0.94724, saving model to model.h5\n",
            "Epoch 32/200\n",
            "47/47 - 40s - loss: 0.8882 - val_loss: 0.9359\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.94724 to 0.93587, saving model to model.h5\n",
            "Epoch 33/200\n",
            "47/47 - 40s - loss: 0.8777 - val_loss: 0.9305\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.93587 to 0.93048, saving model to model.h5\n",
            "Epoch 34/200\n",
            "47/47 - 40s - loss: 0.8673 - val_loss: 0.9201\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.93048 to 0.92012, saving model to model.h5\n",
            "Epoch 35/200\n",
            "47/47 - 40s - loss: 0.8573 - val_loss: 0.9125\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.92012 to 0.91252, saving model to model.h5\n",
            "Epoch 36/200\n",
            "47/47 - 40s - loss: 0.8454 - val_loss: 0.9035\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.91252 to 0.90350, saving model to model.h5\n",
            "Epoch 37/200\n",
            "47/47 - 40s - loss: 0.8355 - val_loss: 0.8936\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.90350 to 0.89362, saving model to model.h5\n",
            "Epoch 38/200\n",
            "47/47 - 40s - loss: 0.8261 - val_loss: 0.8856\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.89362 to 0.88561, saving model to model.h5\n",
            "Epoch 39/200\n",
            "47/47 - 40s - loss: 0.8144 - val_loss: 0.8798\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.88561 to 0.87979, saving model to model.h5\n",
            "Epoch 40/200\n",
            "47/47 - 40s - loss: 0.8038 - val_loss: 0.8685\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.87979 to 0.86846, saving model to model.h5\n",
            "Epoch 41/200\n",
            "47/47 - 41s - loss: 0.7945 - val_loss: 0.8623\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.86846 to 0.86227, saving model to model.h5\n",
            "Epoch 42/200\n",
            "47/47 - 40s - loss: 0.7847 - val_loss: 0.8507\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.86227 to 0.85074, saving model to model.h5\n",
            "Epoch 43/200\n",
            "47/47 - 40s - loss: 0.7735 - val_loss: 0.8442\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.85074 to 0.84418, saving model to model.h5\n",
            "Epoch 44/200\n",
            "47/47 - 40s - loss: 0.7626 - val_loss: 0.8284\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.84418 to 0.82840, saving model to model.h5\n",
            "Epoch 45/200\n",
            "47/47 - 40s - loss: 0.7489 - val_loss: 0.8249\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.82840 to 0.82487, saving model to model.h5\n",
            "Epoch 46/200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inN8hX44_Sui",
        "outputId": "4f10ab4c-4be0-4ca2-be26-ec1468f67353"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        " # load datasets\n",
        "dataset = load_clean_sentences('english-Tagalog-both.pkl')\n",
        "train = load_clean_sentences('english-Tagalog-train.pkl')\n",
        "test = load_clean_sentences('english-Tagalog-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[Tinanong ako ng isang dayuhan kung nasaan ang estasyon], target=[A foreigner asked me where the station was], predicted=[my foreigner asked me where the station was]\n",
            "src=[Ang Inglatera ay katulad ng Hapon sa maraming kadahilanan], target=[England resembles Japan in many respects], predicted=[england resembles japan in many respects]\n",
            "src=[Bakit di mo sabihin sa kanya nang diretso], target=[Why dont you tell her directly], predicted=[why dont you tell her directly]\n",
            "src=[Hindi siya naninigarilyo], target=[He does not smoke], predicted=[he does not smoke]\n",
            "src=[Di masyadong marunong magpranses si Tomas], target=[Tom couldnt speak French well], predicted=[tom couldnt speak french well]\n",
            "src=[Kumanta siya ng kanta], target=[He sang a song], predicted=[he sang a song]\n",
            "src=[Matangkad siya at guwapo], target=[He is tall and handsome], predicted=[he is tall and handsome]\n",
            "src=[Kung anuman iyon hindi ko iyon ginawa], target=[Whatever it is I didnt do it], predicted=[whatever it is i didnt do it]\n",
            "src=[Nagiihaw ng karne si Tomas], target=[Tom is grilling meat], predicted=[tom is grilling meat]\n",
            "src=[Seryoso ka ba], target=[Are you being serious], predicted=[are you serious serious]\n",
            "BLEU-1: 0.769881\n",
            "BLEU-2: 0.729119\n",
            "BLEU-3: 0.709919\n",
            "BLEU-4: 0.631550\n",
            "test\n",
            "src=[Hindi ka kayang tulungan ni Tom], target=[Tom cant help you], predicted=[tom cant help you]\n",
            "src=[Sinong may alam], target=[Who knows], predicted=[who knows]\n",
            "src=[Panoorin niyo ako], target=[Watch me], predicted=[watch me]\n",
            "src=[Ano ang nasa pupitre], target=[What is on the desk], predicted=[what is on the desk]\n",
            "src=[Bumalik si Tomas sa kanyang bayang sinilangan], target=[Tom went back to his hometown], predicted=[tom went back to his hometown]\n",
            "src=[Nagsusuklay siya ng kaniyang buhok], target=[She is brushing her hair], predicted=[she is brushing her hair]\n",
            "src=[Huwag mong alalahanin], target=[Dont worry about it], predicted=[dont worry about it]\n",
            "src=[Baka mas malusog kumain ng papkorn kaysa sa kumain ng poteyto tsip], target=[Its probably healthier to eat popcorn than it is to eat potato chips], predicted=[its probably healthier eat eat popcorn than it is to to potato chips]\n",
            "src=[Tumakbo siya], target=[He ran], predicted=[he ran]\n",
            "src=[Simula alasdos hinihintay na kita], target=[Ive been waiting for you since two oclock], predicted=[ive been waiting for you since two oclock]\n",
            "BLEU-1: 0.632754\n",
            "BLEU-2: 0.589577\n",
            "BLEU-3: 0.582804\n",
            "BLEU-4: 0.506442\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHvk8FnrCysj",
        "outputId": "dabe7fc4-dc80-4048-b13d-7114cb69afdc"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 100:\n",
        "\t\t\tprint('Source=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        " # load datasets\n",
        "dataset = load_clean_sentences('english-Tagalog-both.pkl')\n",
        "train = load_clean_sentences('english-Tagalog-train.pkl')\n",
        "test = load_clean_sentences('english-Tagalog-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "Source=[Tinanong ako ng isang dayuhan kung nasaan ang estasyon], target=[A foreigner asked me where the station was], predicted=[my foreigner asked me where the station was]\n",
            "Source=[Ang Inglatera ay katulad ng Hapon sa maraming kadahilanan], target=[England resembles Japan in many respects], predicted=[england resembles japan in many respects]\n",
            "Source=[Bakit di mo sabihin sa kanya nang diretso], target=[Why dont you tell her directly], predicted=[why dont you tell her directly]\n",
            "Source=[Hindi siya naninigarilyo], target=[He does not smoke], predicted=[he does not smoke]\n",
            "Source=[Di masyadong marunong magpranses si Tomas], target=[Tom couldnt speak French well], predicted=[tom couldnt speak french well]\n",
            "Source=[Kumanta siya ng kanta], target=[He sang a song], predicted=[he sang a song]\n",
            "Source=[Matangkad siya at guwapo], target=[He is tall and handsome], predicted=[he is tall and handsome]\n",
            "Source=[Kung anuman iyon hindi ko iyon ginawa], target=[Whatever it is I didnt do it], predicted=[whatever it is i didnt do it]\n",
            "Source=[Nagiihaw ng karne si Tomas], target=[Tom is grilling meat], predicted=[tom is grilling meat]\n",
            "Source=[Seryoso ka ba], target=[Are you being serious], predicted=[are you serious serious]\n",
            "Source=[Nagsasalita siya nang naglalakad siya], target=[He was talking as he walked], predicted=[he was talking as he walked]\n",
            "Source=[Nagsasalita ka ba ng Pranses arawaraw], target=[Do you speak French every day], predicted=[do you speak french every day]\n",
            "Source=[Sundan mo ako], target=[Follow me], predicted=[follow me]\n",
            "Source=[Magdala ka ng isang balde ng mansanas], target=[Bring a bucket of apples], predicted=[bring a bucket of apples]\n",
            "Source=[Nagpriprito ng itlog si Tomas], target=[Tom is frying an egg], predicted=[tom is frying an egg]\n",
            "Source=[Namagitan si Tom], target=[Tom intervened], predicted=[tom intervened]\n",
            "Source=[Kaya kong sabihin sayo na hindi yun nangyari], target=[I can tell you that it didnt happen], predicted=[i can tell you that it didnt happen]\n",
            "Source=[Lumipad ang uwak], target=[The crow flew away], predicted=[the crow flew away]\n",
            "Source=[Umubo si Tom], target=[Tom coughed], predicted=[tom coughed]\n",
            "Source=[Pwede mo ba akong tulungan], target=[Can you possibly help me], predicted=[can you please help me]\n",
            "Source=[Pinagsamasama niya ang antolohiya ng Hapong alamat para gamitin sa paaralan], target=[He compiled a Japanese folklore anthology for use in schools], predicted=[he compiled a japanese folklore anthology for use in schools]\n",
            "Source=[Walang taong makakaalam ng lahat], target=[Nobody can know everything], predicted=[nobody can know everything]\n",
            "Source=[Gusto ko sila], target=[I like them], predicted=[i like them]\n",
            "Source=[Naiwanan sila sa isang pulong walang tao], target=[They were stranded on a deserted island], predicted=[they were stranded on a deserted island]\n",
            "Source=[Ang Hapon ay malamang na kilalanin sa grupo nilang pinanggagalingan], target=[The Japanese tend to identify themselves with the group they belong to], predicted=[the japanese tend to identify themselves with the the group belong to]\n",
            "Source=[Magisa bang nagtrabaho si Tom], target=[Was Tom working alone], predicted=[was tom working alone]\n",
            "Source=[Hindi niya sinadyang matamaan ako], target=[He hit me by mistake], predicted=[he hit me to hit me]\n",
            "Source=[Siya ang nagbigay ng patunay sa pahayag ko], target=[He gave witness to the truth of my statement], predicted=[he gave witness to the truth of my statement]\n",
            "Source=[Nakauwi na ako], target=[Im home], predicted=[im home]\n",
            "Source=[Gusto niyo po bang umupo], target=[Wont you take a chair], predicted=[would you like to chair]\n",
            "Source=[Pinuri siya ng kanyang guro], target=[Her teacher praised her], predicted=[her teacher praised her]\n",
            "Source=[Mas matangkad ako], target=[I am taller], predicted=[i am taller]\n",
            "Source=[Huwag pong magsalita nang napakabilis], target=[Please dont speak so fast], predicted=[please dont speak so fast]\n",
            "Source=[Nung nasa hayskul ako kumita ako ng ekstrang pera nang nagbebeybisit ako], target=[When I was in high school I used to earn extra money babysitting], predicted=[when i was in high school i used to earn extra money babysitting]\n",
            "Source=[Biro ito tama], target=[This is a joke right], predicted=[this is a joke right]\n",
            "Source=[Kalikasang tao ang maabala ng mga ganoong bagay], target=[It is human nature to be bugged by such things], predicted=[it is human nature to be bugged by such things]\n",
            "Source=[Sumusulat ako sa inyo para ipaalam na hindi ako nakontento], target=[I am writing to express my dissatisfaction], predicted=[i am writing to express my dissatisfaction]\n",
            "Source=[Susubukan kong huwag kang abalahin habang nagaaral ka], target=[Ill try not to disturb you while youre studying], predicted=[ill try not to disturb you while youre studying]\n",
            "Source=[Hindi ako makapaniwala na sumusuko ka na], target=[I cant believe youre giving up], predicted=[i cant believe youre giving up]\n",
            "Source=[Sino ang nagkamali], target=[Who was wrong], predicted=[who was wrong]\n",
            "Source=[Sino ang sasama sa iyo], target=[Whos coming with you], predicted=[whos coming with you]\n",
            "Source=[Alam mo ba iyon], target=[Did you know that], predicted=[did you know that]\n",
            "Source=[Kailangan mong sumipag], target=[You have to work hard], predicted=[you have to work hard]\n",
            "Source=[May bahay na ba si Tom ngayon], target=[Does Tom have a house now], predicted=[does tom have a house now]\n",
            "Source=[Saan ka nanuod ng telebisyon], target=[Where do you watch television], predicted=[where do you watch television]\n",
            "Source=[Gisingin mo kami para sa almusal], target=[Wake us up in time for breakfast], predicted=[wake us up in time for breakfast]\n",
            "Source=[Magiging maayos ang lahat], target=[Everythings going to be all right], predicted=[everythings going to be all right]\n",
            "Source=[Gising pa si Tom], target=[Tom is still awake], predicted=[tom is still awake]\n",
            "Source=[Tinanong niya kung gusto ko ang math], target=[He asked me whether I like math], predicted=[he asked me whether i like math]\n",
            "Source=[Sana mas matangkad ako nang kaunti], target=[I wish I were a little taller], predicted=[i wish i were a little taller]\n",
            "Source=[Dali Mahuhuli na tayo], target=[Come on Well be late], predicted=[come on well be late]\n",
            "Source=[Busog na ako], target=[Im full], predicted=[im full]\n",
            "Source=[Wala ka nang hahanapin pa habang nabubuhay ako], target=[Youll want for nothing while I am alive], predicted=[youll want for nothing while i am alive]\n",
            "Source=[Paano kayo nagkakilala], target=[How did you come to know her], predicted=[how did you come to know her]\n",
            "Source=[Gustunggusto kong naglalaro ng golf], target=[I love to play golf], predicted=[i love to play golf]\n",
            "Source=[Pumunta kami sa Roma na natigilan namin nang isang linggo], target=[We went to Rome where we stayed a week], predicted=[we went to rome where we stayed a week]\n",
            "Source=[Wala tayong pagpipilian], target=[We have no choice], predicted=[we have no choice]\n",
            "Source=[Magsayaw ba tayo], target=[Shall we dance], predicted=[shall we dance]\n",
            "Source=[Mayroon akong takdangaralin], target=[I have homework], predicted=[i have homework]\n",
            "Source=[Sino siya], target=[Who was he], predicted=[who is he]\n",
            "Source=[Nagkakasayihan kami], target=[We are having a good time], predicted=[we are having a good time]\n",
            "Source=[Hindi ito kapanipaniwala], target=[Its incredible], predicted=[this is]\n",
            "Source=[Ako ang asawa ni Tom], target=[Im Toms wife], predicted=[im toms wife]\n",
            "Source=[Masyadong mabigat itong kahon para bitbitin ko], target=[This box is too heavy for me to carry], predicted=[this box is too heavy for me to carry]\n",
            "Source=[Huwag mo kalimutang magdala ng payong], target=[Dont forget to take an umbrella with you], predicted=[dont forget to take an umbrella with you]\n",
            "Source=[Hindi mo alam], target=[Dont you know], predicted=[dont you know]\n",
            "Source=[Nananaginip ba si Tom], target=[Is Tom dreaming], predicted=[is tom dreaming]\n",
            "Source=[Mayroon ka bang lisensya], target=[Do you have a license], predicted=[do you have a license]\n",
            "Source=[Tunay pala yung tsismis], target=[The rumor turned out to be true], predicted=[the rumor turned out to be true]\n",
            "Source=[Ang balita ko nagkasakit ka raw], target=[Ive heard youve been sick], predicted=[ive heard youve been sick]\n",
            "Source=[Huwag mo akong niloloko], target=[Dont try to deceive me], predicted=[dont try to deceive me]\n",
            "Source=[Huwag tumigil], target=[Dont stop], predicted=[dont quit]\n",
            "Source=[Maaari kang pumili kung alinman yung gusto mo], target=[You may choose whichever you want], predicted=[you may choose whichever you want]\n",
            "Source=[Tumutugtog ako ng piano], target=[I play piano], predicted=[i play piano]\n",
            "Source=[Mayroon ka bang lisensya para dyan], target=[Do you have a license for that], predicted=[do you have a license for that]\n",
            "Source=[Kumalma ka], target=[Calm down], predicted=[calm down]\n",
            "Source=[Ngumiti ang lahat], target=[Everybody smiled], predicted=[everybody smiled]\n",
            "Source=[Huwag kalimutan], target=[Dont forget], predicted=[dont forget]\n",
            "Source=[Wala ka], target=[Youre nothing], predicted=[youre nothing]\n",
            "Source=[Nagsayaw sila], target=[They danced], predicted=[they danced]\n",
            "Source=[Dinala niya ako sa isla nang bangka niya], target=[He took me over to the island in his boat], predicted=[he took me over to the island in his boat]\n",
            "Source=[Susulat ka ba kay Tom], target=[Are you going to write to Tom], predicted=[are you going to write to tom]\n",
            "Source=[Sumigaw ako], target=[I screamed], predicted=[i shouted]\n",
            "Source=[Ito ang ospital kung saan ako ipinanganak], target=[This is the hospital where I was born], predicted=[this is the hospital where i was born]\n",
            "Source=[Gusto ko], target=[I like it], predicted=[i like it]\n",
            "Source=[Pinasok na ang data sa kompyuter], target=[The data has been fed into the computer], predicted=[the data has been fed into the computer]\n",
            "Source=[Apat na beses ko na aakyatin ang Bundok ng Fuji kung muli kong aakyatin], target=[I will have climbed Mt Fuji four times if I climb it again], predicted=[i have have climbed mt fuji four times if i climb it again]\n",
            "Source=[Hindi ako babalik], target=[Im not going back], predicted=[i wont come back]\n",
            "Source=[Mas komplikado kaysa sa iyan], target=[Its more complicated than that], predicted=[its more complicated than that]\n",
            "Source=[Pinatay ang tubig], target=[The water has been cut off], predicted=[the water has been cut off]\n",
            "Source=[Nagtanim ako ng rosas sa hardin], target=[I planted roses in the garden], predicted=[i planted roses in the garden]\n",
            "Source=[Kumukulong mainit na ang tsa], target=[The tea is boiling hot], predicted=[the tea is boiling hot]\n",
            "Source=[Simulan ang pagbibilang], target=[Start counting], predicted=[start counting]\n",
            "Source=[Nakita ko si Tom kanina], target=[I saw Tom a while back], predicted=[i saw tom a while back]\n",
            "Source=[Nakalimutan mo nang bata ka noon], target=[Have you forgotten that you were young once], predicted=[have you forgotten that you were young once]\n",
            "Source=[Ito ang aking anakutak], target=[This is my brainchild], predicted=[this is my brainchild]\n",
            "Source=[Nakapagtrabaho siya hanggang 75 taong gulang dahil sa kanyang malusog na katawan], target=[His good health enabled him to work till the age of seventyfive], predicted=[his good health enabled him to work till the age of seventyfive]\n",
            "Source=[Matagal siyang tumitigil bawat pagparini niya], target=[He stays a long time every time he comes], predicted=[he stays a long time every time he comes]\n",
            "Source=[Hindi ako milyonaryo], target=[Im not a millionaire], predicted=[im not a millionaire]\n",
            "Source=[Hindi ako mabubuhay kung wala siya], target=[I cant live without her], predicted=[i cant live without her]\n",
            "BLEU-1: 0.769881\n",
            "BLEU-2: 0.729119\n",
            "BLEU-3: 0.709919\n",
            "BLEU-4: 0.631550\n",
            "test\n",
            "Source=[Hindi ka kayang tulungan ni Tom], target=[Tom cant help you], predicted=[tom cant help you]\n",
            "Source=[Sinong may alam], target=[Who knows], predicted=[who knows]\n",
            "Source=[Panoorin niyo ako], target=[Watch me], predicted=[watch me]\n",
            "Source=[Ano ang nasa pupitre], target=[What is on the desk], predicted=[what is on the desk]\n",
            "Source=[Bumalik si Tomas sa kanyang bayang sinilangan], target=[Tom went back to his hometown], predicted=[tom went back to his hometown]\n",
            "Source=[Nagsusuklay siya ng kaniyang buhok], target=[She is brushing her hair], predicted=[she is brushing her hair]\n",
            "Source=[Huwag mong alalahanin], target=[Dont worry about it], predicted=[dont worry about it]\n",
            "Source=[Baka mas malusog kumain ng papkorn kaysa sa kumain ng poteyto tsip], target=[Its probably healthier to eat popcorn than it is to eat potato chips], predicted=[its probably healthier eat eat popcorn than it is to to potato chips]\n",
            "Source=[Tumakbo siya], target=[He ran], predicted=[he ran]\n",
            "Source=[Simula alasdos hinihintay na kita], target=[Ive been waiting for you since two oclock], predicted=[ive been waiting for you since two oclock]\n",
            "Source=[Talagang di ako makalangoy], target=[I cant swim at all], predicted=[i cant swim at all]\n",
            "Source=[Lumapit ka], target=[Come closer], predicted=[come closer]\n",
            "Source=[Gaanong kataas yang tore], target=[How high is that tower], predicted=[how high is that tower]\n",
            "Source=[Baka nasa kusina si Tom pero ewan ko], target=[Tom could be in the kitchen but I dont know], predicted=[tom could be in the kitchen but i dont know]\n",
            "Source=[Magkapareho ba sila], target=[Are they all the same], predicted=[are they all the same]\n",
            "Source=[Sumeryoso ka], target=[Get serious], predicted=[get serious]\n",
            "Source=[Inaantok ako], target=[I feel sleepy], predicted=[i feel]\n",
            "Source=[Makulit siyang magtanong sa akin], target=[He annoys me with questions], predicted=[he annoys me with questions]\n",
            "Source=[Wala si Tomas mapuntahan], target=[Tom had no place to go], predicted=[tom had no place to go]\n",
            "Source=[Makikiraan po], target=[Make way please], predicted=[make way please]\n",
            "Source=[May pusa siya Puti ang pusa niya], target=[She has a cat The cat is white], predicted=[she has a cat the cat is white]\n",
            "Source=[Ano bang sinasabi mo], target=[What are you talking about], predicted=[what are you talking about]\n",
            "Source=[Ayaw ko nang pagusapan ito], target=[I dont want to discuss this anymore], predicted=[i dont want to discuss this anymore]\n",
            "Source=[Pupunta ako sa sine kung may oras ako], target=[I would go to the movies if I had the time], predicted=[i would to to the movies if i had the time]\n",
            "Source=[Imposibleng matapos ko yung trabaho sa loob ng isang araw], target=[It is impossible for me to finish the work in a day], predicted=[it is impossible for me to finish the work in a day]\n",
            "Source=[Gusto ko siya], target=[I like him], predicted=[i like him]\n",
            "Source=[Gusto ko lahat sila], target=[I like all of them], predicted=[i like all of them]\n",
            "Source=[Kailan ito matatapos], target=[When was it finished], predicted=[when was it finished]\n",
            "Source=[Dinagan niya ang kanyang mukha sa bintana ng tindahan], target=[He pressed his face against the shop window], predicted=[he pressed his face against the shop window]\n",
            "Source=[Tinuloy ko ang pagbabasa], target=[I went on reading], predicted=[i went on reading]\n",
            "Source=[Hindi ka ba lumalangoy], target=[Dont you swim], predicted=[dont you swim]\n",
            "Source=[Gaano tumagal iyon], target=[How long did it last], predicted=[how long did it last]\n",
            "Source=[Hindi ko maalala ang pangalan niya], target=[I cant remember his name], predicted=[i cant remember his name]\n",
            "Source=[Tawagan mo ako bago ka umalis], target=[Call me before you leave], predicted=[call me before you leave]\n",
            "Source=[Di ako pupunta sa paaralan bukas], target=[I will not go to school tomorrow], predicted=[i will not go to school tomorrow]\n",
            "Source=[Akala ko gagawa si Tom ng almusal], target=[I thought Tom would make breakfast], predicted=[i thought tom would make breakfast]\n",
            "Source=[Balak kong pumaroon], target=[I had intended to go there], predicted=[i had intended to go there]\n",
            "Source=[Nagintay ako], target=[I waited], predicted=[i waited]\n",
            "Source=[Ayusin mo to], target=[Fix this], predicted=[fix this]\n",
            "Source=[Isip kong talagang mabait na tao ka], target=[I think youre a really nice guy], predicted=[i think youre a really nice guy]\n",
            "Source=[Walang mas importante sa buhay kundi ang kalusugan], target=[Nothing is more important in life than health], predicted=[nothing is more important in life than health]\n",
            "Source=[Mahal ako ni Tom], target=[Tom loves me], predicted=[tom loves me]\n",
            "Source=[Nitong umaga lamang binayaran ni Tom ang utang niya kay Mary], target=[Just this morning Tom repaid Mary the money he owed her], predicted=[just this morning tom repaid the the he he owed her]\n",
            "Source=[Manganganak siya sa July], target=[She will give birth in July], predicted=[she will give birth in july]\n",
            "Source=[Ninanais kita], target=[I want you], predicted=[i want you]\n",
            "Source=[Magaling na tula iyan], target=[Thats a great poem], predicted=[thats a great poem]\n",
            "Source=[Gusto ko ang football], target=[I like football], predicted=[i like football]\n",
            "Source=[Nagaway sila], target=[They quarreled], predicted=[they quarreled]\n",
            "Source=[Ang tren ay dumating sa tamang oras], target=[The train arrived on time], predicted=[the train arrived on time]\n",
            "Source=[Ang Ingles ay isang wikang pandaigdig], target=[English is an international language], predicted=[english is an international language]\n",
            "Source=[Ikaw ay mayaman], target=[You are rich], predicted=[you are rich]\n",
            "Source=[Anong paborito mong podcast], target=[Whats your favorite podcast], predicted=[whats your favorite podcast]\n",
            "Source=[Takot ka bang mamatay], target=[Are you afraid of death], predicted=[are you afraid of death]\n",
            "Source=[Tumira siyang magisa sa bahay kubo], target=[She was living alone in a hut], predicted=[she was living alone in a hut]\n",
            "Source=[Nagtanong siya tungkol sa kinaroroonan ng bahay], target=[She asked about the location of the house], predicted=[she asked about the location of the house]\n",
            "Source=[Nasaan na yung iba pang files], target=[Where are the rest of the files], predicted=[where are the rest of the files]\n",
            "Source=[Hindi ako ang dapat mong tanungin], target=[Youre asking the wrong person], predicted=[youre asking the wrong person]\n",
            "Source=[Kakaalis lang niya], target=[She just left], predicted=[she just left]\n",
            "Source=[May nangyari sa kotse ko], target=[Something has happened to my car], predicted=[something has happened to my car]\n",
            "Source=[Dumating si Tom], target=[Tom came], predicted=[tom came]\n",
            "Source=[Hindi mo maintindihan], target=[Dont you understand], predicted=[dont you understand]\n",
            "Source=[Sumeryoso ka nga], target=[Get serious], predicted=[get serious]\n",
            "Source=[Nakakatuwa ang pagsasalita ng Ingles], target=[Speaking English is a lot of fun], predicted=[speaking english is a lot of fun]\n",
            "Source=[Anong ginawa mo], target=[Whatve you done], predicted=[what did you make]\n",
            "Source=[Hindi mo ba abot ang mga libro sa shelf], target=[Cant you reach the book on the shelf], predicted=[cant you reach the book on the shelf]\n",
            "Source=[Inalis nina Tom at Mary ang bangka sa tubig], target=[Tom and Mary took the boat out of the water], predicted=[tom and mary took the boat out of the water]\n",
            "Source=[Sabihin mo sa akin ang pangalan ng ika9 na buwan], target=[Tell me the name of the ninth month], predicted=[tell me the name of the ninth month]\n",
            "Source=[Mahahanap mo ang libro sa silidaklatan], target=[Youll find the book in the library], predicted=[youll find the book in the library]\n",
            "Source=[Anong ginawa mo], target=[What did you do], predicted=[what did you make]\n",
            "Source=[Magintay ka sa labas], target=[Wait outside], predicted=[wait outside]\n",
            "Source=[Ang aming kapalaran ay depende sa iyong mga desisyon], target=[Our fate depends on your decisions], predicted=[our fate depends on your decisions]\n",
            "Source=[Di ka ba inaantok], target=[Arent you sleepy], predicted=[arent you sleepy]\n",
            "Source=[Ipinakilala niya sa akin ang anak niyang babae], target=[He introduced his daughter to me], predicted=[he introduced his daughter to me]\n",
            "Source=[Bumababa ang kalidad ng bigas], target=[The quality of rice is going down], predicted=[the quality of rice is going down]\n",
            "Source=[Pumasok ang bata sa bintana], target=[The boy got in through the window], predicted=[the boy got in through the window]\n",
            "Source=[Parang hindi naman kapanipaniwala yan], target=[I highly doubt that], predicted=[i highly doubt that]\n",
            "Source=[Walang nakakaalala], target=[Nobody remembers], predicted=[nobody remembers]\n",
            "Source=[Kung may panahon ako magaaral ako ng Pranses], target=[If I had time I would study French], predicted=[if i had time i would study french]\n",
            "Source=[Naubos niya ang pera niya], target=[He exhausted his money], predicted=[he ran out of money]\n",
            "Source=[Narangha ang kurbata ko], target=[My tie is orange], predicted=[my tie is orange]\n",
            "Source=[Bihira lang umulan dito], target=[It hardly ever rains here], predicted=[it hardly ever rains here]\n",
            "Source=[Nakakita ka na ba ng koala], target=[Have you ever seen a koala], predicted=[have you ever seen a koala]\n",
            "Source=[Ang bawat taoy tumayo], target=[Everyone stood], predicted=[everyone stood]\n",
            "Source=[Talagang napakahirap pumili ng tamang kulay], target=[Its really hard to choose the right color], predicted=[its really hard to choose the right color]\n",
            "Source=[Dadating siya nang alas otso nang pinakahuli], target=[He will arrive by eight at the latest], predicted=[he will arrive by eight at the latest]\n",
            "Source=[Inilagay niya ang kahon sa mesa], target=[He put the box on the table], predicted=[he put the box on the table]\n",
            "Source=[Isara ang pinto], target=[Shut the door], predicted=[close the door]\n",
            "Source=[Padalhan mo ako ng postcard], target=[Send me a postcard], predicted=[send me a postcard]\n",
            "Source=[Ang nayon ay binukod ng malakas na bagyo], target=[The village was isolated by the heavy storm], predicted=[the village was isolated by the heavy storm]\n",
            "Source=[Kinausap ko ang mga kaibigan], target=[I talked to friends], predicted=[i talked to friends]\n",
            "Source=[Umiyak si Tom], target=[Tom cried], predicted=[tom cried]\n",
            "Source=[Dapat hindi ka na nagparini sa simula e], target=[You shouldnt have come here to begin with], predicted=[you shouldnt have come here to begin with]\n",
            "Source=[Umuulan nanaman], target=[Its raining again], predicted=[its raining again]\n",
            "Source=[Tapusin natin ang laro], target=[Lets stay until the end of the game], predicted=[lets stay until the end of the game]\n",
            "Source=[Kailangan kitang makausap], target=[I need to talk to you], predicted=[i need to talk to you]\n",
            "Source=[Natapos mo na bang basahin ang dyaryo ngayong araw], target=[Have you finished reading todays paper yet], predicted=[have you finished reading todays paper yet]\n",
            "Source=[Kung hindi namatay ang tatay ko sa digmaan lagpas sisenta na sana siya ngayon], target=[If my father had not been killed in the war he would be over sixty years old now], predicted=[if my father had been been killed in in he he would be be sixty years old now]\n",
            "Source=[Nagsayaw si Tom], target=[Tom danced], predicted=[tom danced]\n",
            "Source=[Sa tingin ko hindi pa ako handa], target=[I dont think Im ready], predicted=[i dont think im ready]\n",
            "Source=[TagaBrasil ako], target=[I am from Brazil], predicted=[i am from brazil]\n",
            "BLEU-1: 0.632754\n",
            "BLEU-2: 0.589577\n",
            "BLEU-3: 0.582804\n",
            "BLEU-4: 0.506442\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}